<div style="text-align:center">
<!-- <img src="https://big-cheng.com/k2/k2.png" alt="k2-logo" width="200"/> -->
<h2>ğŸ“ˆ CFGPT: Chinese Financial Assistant with Large Language Model</h2>
</div>

<a href='https://arxiv.org/abs/2309.10654'><img src='https://img.shields.io/badge/Paper-ArXiv-C71585'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(pt)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20LoRA)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20Full)-red'></a> 

English | [ç®€ä½“ä¸­æ–‡](README-zh.md)

# Introduction

We introduce **CFGPT**, an open-source language model trained by firstly further pretraining general LLMs on collected and cleaned Chinese finance text data (CFData-pt), including financial domain-specific data (announcement, finance articles, finance exams, finance news, finance research papers) and general data (Wikipedia), and secondly fine-tuning with knowledge-intensive instruction tuning data (CFData-sft). 
As for preliminary evaluation, we use CFBenchmark-Basic. 
CFGPT outperforms the baselines on objective and subjective tasks compared to several baseline models with similar parameters. 

In this repository, we will share the following models and code.

- We release CFGPT1 (7B) in three parts:
    - [Pretrained Model](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B): Full model weights after further pretraining with the chinese finance text corpus to comply with the InternLM model license. 
    - [Supervised Finetuned Model (Lora)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA): Adapter model weights trained by PEFT (LoRA).
    - [Supervised Finetuned Model (Full)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full): Full model trained weights based on the pretrained model.

- We release the [CFBenchmark](https://github.com/TongjiFinLab/CFBenchmark), a Chinese financial assistant benhmark for large language model. The basic version of CFBenchmark includes 3917 financial texts spanning three aspects and eight tasks, for the evaluation of the financial text processing capability of LLMs in Chinese financial market.

- We release the code of further pretrain and instruction tuning of CFGPT.

- We further provide several samples about our CFData-sft.

***The following is the overview of training CFGPT:***
<div align="center">
<img align="center" src=./figs/CFGPT-Training.svg width="100%"/>
</div>

# Content

- [Quick Start](#quick-start)
- [Data](#data)
- [Code](#code)
- [Cases](#cases)
- [Benchmark](#benchmark)
- [Acknowledgements](#acknowledgements)
- [To-Do List](#to-do-list)
- [License](#license)
- [Citation](#citation-arxiv)

# Quick Start

**1. Prepare the code and the environment**

Clone our repository, create a Python environment, and activate it via the following command
```bash
git clone https://github.com/TongjiFinLab/CFGPT.git
cd CFGPT
conda create -n env_name python=3.10   
source activate env_name 
pip install -r requirements.txt
```

**2. Prepare the pretrained CFGPT1**

The CFGPT1 consists of three parts: a pretrain model, continued pretraining InternLM-7B on our CFData-pt, an adapter model (trained via PEFT on our CFData-sft), and a Full-finetuned model trained base on the pretrain model.

|Pretrain model|Adapter model|Full SFT Model|
|:-:|:-:|:-:|
 [CFGPT1-pt-7B](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B)|[CFGPT1-sft-7B-lora](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA)|[CFGPT1-sft-7B-full](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full)|

**3. Use CFGPT1-sft-7B-LoRA**

```python
from transformers import AutoModel, AutoTokenizer
from peft import PeftModel
base_model = 'TongjiFinLab/CFGPT1-pt-7B'
lora_weights = 'TongjiFinLab/CFGPT1-sft-7B-LoRA'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = PeftModel.from_pretrained(
    model,
    lora_weights,
    device_map=device_map,
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

**4. Use CFGPT1-sft-7B-Full**

```python
from transformers import AutoModel, AutoTokenizer
base_model = 'TongjiFinLab/CFGPT1-sft-7B-Full'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

- **More detail are in `./code/test`**

# Data

In this repo, we share the samples of CFData:
- CFData: `./data`

    The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised finetuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decisionmaking with 1.5M instruction pairs and 1.5B tokens in total.


## Further pretrain

The pre-training dataset consists of 591 million documents and 193 billion tokens, including six sub-datasets
* CFData-CP (6.24%): 39 thousand corporate prospectus with 13 billion tokens;
* CFData-CA (12.28%): 6 million corporate announcements with 17 billion tokens; 
* CFData-RR (2.51% ): 392 thousand research reports with 3 billion tokens; 
* CFData-FN (18.70%): 82 million financial news with 26 billion tokens; 
* CFData-SM (60.15%): 495 million social medias and 84 billion tokens; 
* CFData-Wiki (0.09%): 255 thousand Wikipedia content with 137 million tokens.

We sample a financial text sub-corpus from CFData-pt for further pretraining on InternLM-7B consists of 13.7 billion tokens from a large amount of Chinese financial data and analytics and a small amount of general-purpose text, such as announcements, research reports, social media content, financial news articles, and Wikipedia. And they were mainly collected by ourselves.

## Supervised Finetuning

The supervised fine-tuning dataset consist 1.6 million instructions pairs and 1.5 billion tokens, including six financial tasks: 
* CFData-SA (5.69% ): 120 thousand instances with 86 million tokens for sentiment analysis; 
* CFData-RS (50.60%): 369 thousand instances and 765 million tokens for report summary; 
* CFData-ED (22.69% ): 490 thousand instances with 343 million tokens for event detection; 
* CFData-TD (12.37%): 369 thousand instances and 187 million tokens for topic decomposition; 
* CFData-QA (0.39%): 12 thousand instances and 6 million tokens for question-answering; 
* CFData-SP (8.27%): 212 thousand instances and 125 million tokens for stock moving prediction.

We employ high-quality domain specific data to achieve finance domain adaptation during supervised finetuing. The dataset includes six financial datasets to reflect different aspects of financial analysis and decision-making, which include sentiment analysis, event detection, report summarization, topic decomposition, question answering, and stock movement prediction. 
CFData-sft provides much text information in the financial domain, allowing a FinLLM to learn from different of sources.
Considering requirement in reality, we reform these financial supervised finetuning dataset into ten tasks.

The details are as follows:
| Task | Task Description | Dataset | Size |
| - | - | - | - |
| Sentiment | Identify the sentiment associated with financial document | CFData-SA | 13K |
| Summary | Generate a content summary based on the provided financial document | CFData-RS | 18K |
| Risk | Generate risk alerts based on the provided financial document | CFData-RS | 20K |
| Suggestion | Generate investment recommendations based on the provided financial document | CFData-RS | 18K |
| Event | Identify the event categories associated with financial document | CFData-ED | 12K |
| Industry | Identify the industry categories associated with financial document | CFData-ED | 14K |
| Company | Identify the company names associated with financial document | CFData-ED | 12K |
| Product | Identify the product names associated with financial document | CFData-ED | 21K |
| Exam | Answer true-false questions related to finance question | CFData-QA | 16K |
| Stock | Predict stocks future movement |  CFData-SP | 15K |

# Code

## Further Pretrain

The training script is **`./code/train/pretrain`**

```bash
deepspeed --include localhost:0,1,2,3,4,5,6,7 --master_port 60002 bf_16_parallel_train.py --config bf_16_parallel_train.yml > bf_16_parallel_train.log 2>&1
```

<!-- ![loss curve](https://big-cheng.com/k2/loss_curve.png) -->
<div align="center">
<img align="center" src=./figs/CFGPT-Training-loss.svg width="100%"/>
</div>

The trainer parameters we use are in **`./code/train/pretrain/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/further/pretrain/model
dataset: path/to/your/further/pretrain/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./bf_16_parallel_train
logging_steps: 10
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 1000
save_steps: 1000
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
remove_unused_columns: 0
```

The deepspeed parameters we use are in **`./code/train/pretrain/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,
    "optimizer": {
        "type": "AdamW",
        "params": {
          "lr": "auto",
          "betas": "auto",
          "eps": "auto",
          "weight_decay": 0.01
          }
        },
     "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": "auto",
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 1,
        "reduce_bucket_size": 5e8
    }
}
```

## Supervised Finetuning

The training script is in **`./code/train/lora`**. Here we use the lora-bf16 as illustrations.

```bash
deepspeed --include localhost:6,7 --master_port 60005 lora_bf_16_parallel_train.py --config lora_bf_16_parallel_train.yml > lora_bf_16_parallel_train.log 2>&1
```

The trainer parameters we use are in **`./code/train/lora/bf16/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/supervised/finetuning/model
dataset: path/to/your/supervised/finetuning/dataset
dataset_eval: path/to/your/evaluate/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./lora_bf_16_parallel_train
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 500
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
save_steps: 500
evaluation_strategy: steps
eval_steps: 100
logging_steps: 10
remove_unused_columns: 0

# lora setting
rank: 64
lora_alpha: 16
lora_dropout: 0.05
target_modules: ['k_proj', 'o_proj', 'down_proj', 'v_proj', 'q_proj', 'gate_proj', 'up_proj']
bias: 'none'

# restart info
resume_from_checkpoint: null
```

The deepspeed parameters we use are in **`./code/train/lora/bf16/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,

    "optimizer": {
      "type": "AdamW",
      "params": {
        "lr": "auto",
        "betas": "auto",
        "eps": "auto",
        "weight_decay": "auto"
        }
      
      },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
      "enabled": true
    },
    "zero_optimization": {
        "stage": 0
    }
}
```


# Cases

- **Sentiment:**
    - **Question:** 
        >ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚
    - **Answer:** 
        >ç§¯æ

- **Summary:**
    - **Question:** 
        >ä½œä¸ºä¸€åé‡‘èæŠ•èµ„åˆ†æä¸“å®¶ï¼Œä½ éœ€è¦åˆ†æå¹¶æ€»ç»“æ¥è‡ªä¸åŒåˆ¸å•†çš„ç ”ç©¶æŠ¥å‘Šã€‚è¯·ä½ çªå‡ºè¯¥ç ”æŠ¥çš„åˆ›æ–°ç‚¹å’Œå¸‚åœºæ´è§ã€‚ç ”æŠ¥å†…å®¹ï¼šäº‹ä»¶ï¼šå…¬å¸4æœˆ26æ—¥å‘å¸ƒ2022å¹´å¹´åº¦æŠ¥å‘ŠåŠ2023å¹´ä¸€å­£æŠ¥ï¼Œ2022å¹´å®ç°è¥æ”¶10.71äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿21.89%ï¼›å½’æ¯å‡€åˆ©æ¶¦5.26äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿19.95%ï¼›æ‰£éå‡€åˆ©æ¶¦5.05äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿16.32%ã€‚2023å¹´Q1å®ç°è¥æ”¶2.00äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™22.90%ï¼Œç¯æ¯”ä¸‹é™31.59%ï¼›å½’æ¯å‡€åˆ©æ¶¦0.75äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™38.88%ï¼Œç¯æ¯”ä¸‹é™48.40%ï¼›æ‰£éå‡€åˆ©æ¶¦0.71äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™41.52%ï¼Œç¯æ¯”ä¸‹é™47.81%ã€‚
        2022å¹´ï¼Œå…¬å¸ç§¯æä¼˜åŒ–äº§å“ç»“æ„ï¼ŒæŒç»­åŠ å¼ºåœ¨æ–°èƒ½æºå’Œå·¥ä¸šé¢†åŸŸçš„æµ‹è¯•å¸ƒå±€ï¼Œéƒ¨åˆ†æŠµæ¶ˆäº†æ¶ˆè´¹ç±»èŠ¯ç‰‡éœ€æ±‚ä¸‹æ»‘çš„å½±å“ï¼ŒåŠ©åŠ›å…¨å¹´ä¸šç»©ç¨³æ­¥å¢é•¿ã€‚2023å¹´Q1å—è¡Œä¸šæ³¢åŠ¨åŠäº§å“ç»“æ„å˜åŒ–å½±å“ï¼Œä¸»è¥ä¸šåŠ¡æ¯›åˆ©æœ‰æ‰€ä¸‹é™ï¼›åŒæ—¶å…¬å¸å¢åŠ äº†å¸‚åœºå¼€æ‹“å’Œç ”å‘æŠ•å…¥ï¼Œè‡´ä½¿å‡€åˆ©æ¶¦çŸ­æœŸæ‰¿å‹ã€‚å…¬å¸2022å¹´æ¯›åˆ©ç‡ä¸º76.88%ï¼ŒåŒæ¯”ä¸‹é™3.34pctï¼›å‡€åˆ©ç‡ä¸º49.16%ï¼ŒåŒæ¯”ä¸‹é™0.80pctã€‚2023å¹´Q1æ¯›åˆ©ç‡ä¸º69.68%ï¼ŒåŒæ¯”ä¸‹é™10.35pctï¼Œç¯æ¯”ä¸‹é™6.57pctï¼›å‡€åˆ©ç‡ä¸º37.33%ï¼ŒåŒæ¯”ä¸‹é™9.76pctï¼Œç¯æ¯”ä¸‹é™12.17pctã€‚è´¹ç”¨æ–¹é¢ï¼ŒQ1é”€å”®ã€ç®¡ç†ã€ç ”å‘ã€è´¢åŠ¡è´¹ç”¨ç‡åˆ†åˆ«ä¸º14.28%/7.15%/16.58%/-3.49%ï¼ŒåŒæ¯”å˜åŠ¨åˆ†åˆ«ä¸º5.73/0.88/6.40/-1.23pctã€‚
        å…¬å¸ä¸ºå›½å†…æ¨¡æ‹Ÿå’Œæ··åˆæµ‹è¯•é¢†åŸŸçš„ä¸»åŠ›æµ‹è¯•å¹³å°ä¾›åº”å•†ï¼ŒåŒæ—¶ä¹Ÿåœ¨åˆ†ç«‹å™¨ä»¶å’ŒåŠŸç‡ç±»å™¨ä»¶æµ‹è¯•é¢†åŸŸå–å¾—è‰¯å¥½è¿›å±•ã€‚å…¬å¸STS8300æœºå‹ä¸»è¦åº”ç”¨äºæ›´é«˜å¼•è„šæ•°ã€æ›´é«˜æ€§èƒ½ã€æ›´å¤šå·¥ä½çš„ç”µæºç®¡ç†ç±»å’Œæ··åˆä¿¡å·é›†æˆç”µè·¯æµ‹è¯•ï¼Œäº§å“çš„å¹³å°åŒ–è®¾è®¡ä½¿å…¶å…·å¤‡è‰¯å¥½çš„å¯æ‰©å……æ€§å’Œå…¼å®¹æ€§ï¼Œå¯ä»¥æ›´å¥½é€‚åº”è¢«æµ‹è¯•èŠ¯ç‰‡çš„æ›´æ–°å’Œè¿­ä»£ã€‚2022å¹´ï¼ŒSTS8300å¹³å°çš„å‡ºè´§é‡ä¿æŒè¾ƒé«˜å¢é€Ÿï¼Œè£…æœºé‡ç¨³å®šå¢åŠ ï¼Œå®¢æˆ·ç”Ÿæ€åœˆæ„å»ºè¿›å±•é¡ºåˆ©ï¼Œå†…éƒ¨èµ„æºæ¿å¡åŠ é€Ÿè¿­ä»£ï¼Œåº”ç”¨èŒƒå›´ä¸æ–­æ‹“å±•ã€‚å…¬å¸æ­£åœ¨ç ”å‘å’Œå‚¨å¤‡æ–°ä¸€ä»£å¹³å°å‹æµ‹è¯•è®¾å¤‡ï¼Œè¯¥è®¾å¤‡å°†æ‹¥æœ‰æ›´å¿«çš„æµ‹è¯•é¢‘ç‡å’Œæ›´é«˜çš„æµ‹è¯•æ•ˆç‡ï¼Œå¯è¦†ç›–çš„æµ‹è¯•èŒƒå›´æ›´å¤§ã€‚éšç€STS8300å¸‚åœºä»½é¢çš„æŒç»­æå‡ï¼Œæ–°å“ç ”å‘ç¨³æ­¥æ¨è¿›ï¼Œæœ‰æœ›æ‰“å¼€ä¸šç»©æˆé•¿å¤©èŠ±æ¿ã€‚
        å…¬å¸åœ¨ä¸æ–­å¤¯å®æ¨¡æ‹Ÿå’Œæ•°æ¨¡æ··åˆé¢†åŸŸçš„ä¼˜åŠ¿çš„åŒæ—¶ï¼Œå¯¹ç¬¬ä¸‰ä»£åŠå¯¼ä½“æµ‹è¯•é¢†åŸŸã€åŠŸç‡æ¨¡å—æµ‹è¯•ä»¥åŠSoCç­‰æ–°å…´é¢†åŸŸè¿›è¡Œäº†å‰ç»å¸ƒå±€ã€‚å½“å‰å…¬å¸åœ¨æ°®åŒ–é•“æµ‹è¯•é¢†åŸŸå·²å–å¾—é¢†å…ˆä¼˜åŠ¿ï¼Œ2022å¹´åœ¨å›½å®¶åŒç¢³æ”¿ç­–æ¨åŠ¨ä¸‹ï¼Œæ–°èƒ½æºæ±½è½¦ã€å…‰ä¼äº§ä¸šå‘ˆç°è¾ƒé«˜çš„æ™¯æ°”åº¦ï¼Œå…¬å¸å¤§åŠŸç‡IGBTå’Œç¢³åŒ–ç¡…æµ‹è¯•é¢†åŸŸå¼€å§‹é€æ¸æ”¾é‡ã€‚å®¢æˆ·æ‹“å±•æ–¹é¢ï¼Œå…¬å¸å·²è¿›å…¥å›½é™…å°æµ‹å¸‚åœºä¾›åº”å•†ä½“ç³»ï¼Œåœ¨ä¸­å›½å°æ¹¾ã€ä¸œå—äºšã€æ—¥æœ¬ã€éŸ©å›½ã€æ¬§æ´²ã€ç¾å›½ã€å—éå’ŒåŒ—éç­‰å›½å®¶æˆ–åœ°åŒºå‡æœ‰è£…æœºï¼›å…¬å¸å¯¹å›½å†…çš„è®¾è®¡å…¬å¸å’ŒIDMä¼ä¸šä¿æŒå…¨é¢è¦†ç›–ï¼Œç¡®ä¿åœ¨é•¿æœŸç«äº‰ä¸­ä¿æŒé¢†å…ˆåœ°ä½ï¼ŒåŒæ—¶æŒç»­æ‹“å±•æµ·å¤–å®¢æˆ·ï¼Œå¦‚æ„æ³•åŠå¯¼ä½“ã€å®‰æ£®ç¾ã€å®‰ä¸–åŠå¯¼ä½“ç­‰å‡å·²æˆä¸ºå…¬å¸å®¢æˆ·ã€‚
        é¦–æ¬¡è¦†ç›–ï¼Œç»™äºˆâ€œå¢æŒâ€è¯„çº§ï¼šå…¬å¸ä¸»è¥ä¸šåŠ¡ä¸ºåŠå¯¼ä½“è‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»Ÿçš„ç ”å‘ï¼Œç”Ÿäº§å’Œé”€å”®ï¼Œä¸»è¦äº§å“ä¸ºåŠå¯¼ä½“è‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»ŸåŠæµ‹è¯•ç³»ç»Ÿé…ä»¶ã€‚å…¬å¸æ‹¥æœ‰æ¨¡æ‹Ÿã€æ•°æ¨¡æ··åˆã€åˆ†ç«‹å™¨ä»¶ä»¥åŠåŠŸç‡æ¨¡å—ç­‰æµ‹è¯•é¢†åŸŸçš„è¯¸å¤šæ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬V/Iæºã€ç²¾å¯†ç”µå‹ç”µæµæµ‹é‡ã€å®½ç¦å¸¦åŠå¯¼ä½“æµ‹è¯•å’Œæ™ºèƒ½åŠŸç‡æ¨¡å—æµ‹è¯•ç­‰ï¼ŒåŒæ—¶å¯†åˆ‡è·Ÿè¸ªåŠå¯¼ä½“è¡Œä¸šçš„å‘å±•æ–¹å‘ï¼Œä¸æ–­ä¸ºå®¢æˆ·æ¨å‡ºåŠŸèƒ½æ›´å…¨ã€ç²¾åº¦æ›´é«˜ã€é€Ÿåº¦æ›´å¿«çš„æµ‹è¯•è®¾å¤‡ã€‚æˆ‘ä»¬çœ‹å¥½å…¬å¸åœ¨ç¬¬ä¸‰ä»£åŠå¯¼ä½“ã€åŠŸç‡ã€SoCç­‰æ–°å…´é¢†åŸŸçš„å…ˆå‘ä¼˜åŠ¿ï¼Œéšç€æ–°å“çš„æŒç»­ç ”å‘å’Œæ¨å‡ºï¼Œä»¥åŠæµ·å†…å¤–å®¢æˆ·çš„æŒç»­æ‹“å±•ï¼Œå…¬å¸ä¸šç»©æœ‰æœ›å®ç°é•¿æœŸç¨³å®šå¢é•¿ã€‚é¢„è®¡å…¬å¸2023-2025å¹´å½’æ¯å‡€åˆ©æ¶¦åˆ†åˆ«ä¸º6.13äº¿å…ƒã€8.16äº¿å…ƒã€10.56äº¿å…ƒï¼ŒEPSåˆ†åˆ«ä¸º6.73ã€8.96ã€11.59å…ƒï¼ŒPEåˆ†åˆ«ä¸º39Xã€29Xã€23Xï¼Œç»™äºˆâ€œå¢æŒâ€è¯„çº§ã€‚
    - **Answer:** 
        >é«˜åŸºæ•°èƒŒæ™¯ä¸‹, Q1ç›ˆåˆ©èƒ½åŠ›ç•¥é™ï½œæ–°äº§å“æœ‰åºæ¨è¿›, STS8300é”€é‡å†åˆ›ä½³ç»©ï½œèšç„¦ç»†åˆ†èµ›é“, æ‹“å®½æˆé•¿ç©ºé—´ï½œè®¢å•é‡Šæ”¾å åŠ ä¸‹æ¸¸ä¿®å¤, å…¬å¸ä¸šç»©æ˜¾è‘—å›æš–ï½œé«˜é˜¶æœºå‹çš„å‡ºè´§å¸¦åŠ¨æ”¶å…¥æ°´å¹³ä¸Šå‡, æ–°äº§å“é€æ­¥æˆç†Ÿå¸¦æ¥åˆ©æ¶¦å¼¹æ€§ï½œä¹˜é£æ–°èƒ½æºèµ›é“ä¸œé£, å¤šå…ƒåŒ–çš„æˆ˜ç•¥å¸ƒå±€è¿æ¥æ”¶è·

- **Risk:**
    - **Question:** 
        >ä½ æ˜¯ä¸€ä½é‡‘èæŠ•èµ„ä¸“å®¶ï¼Œè¯·åˆ†æè¿™ä»½ç ”æŠ¥ä¸­å¯èƒ½å­˜åœ¨çš„æŠ•èµ„é£é™©ï¼ŒåŒ…æ‹¬å¸‚åœºå˜åŠ¨ã€æ”¿ç­–è°ƒæ•´ç­‰æ–¹é¢çš„é£é™©ã€‚
        ç ”æŠ¥å†…å®¹ï¼š 
        ã€€ã€€å…¬å¸å…¬å‘Šç§°æ‹Ÿä»¥è‡ªæœ‰èµ„é‡‘é¢„è®¡å›è´­å…¬å¸è‚¡ä»½ä¸è¶… 450 ä¸‡è‚¡ï¼Œå å…¬å¸ç›®å‰å·²å‘è¡Œæ€»è‚¡æœ¬ 2.01%ï¼Œå›è´­é‡‘é¢åœ¨ 7000 ä¸‡å…ƒè‡³ 13500 ä¸‡å…ƒä¹‹é—´ï¼Œå›è´­ä»·æ ¼ä¸è¶… 30 å…ƒ/è‚¡ï¼Œå›è´­è‚¡ä»½ç”¨äºè‚¡æƒæ¿€åŠ±ï¼Œå›è´­æœŸé™è‡ªè‘£äº‹ä¼šå®¡è®®é€šè¿‡å›è´­è‚¡ä»½æ–¹æ¡ˆä¹‹æ—¥èµ·ä¸è¶…è¿‡ 6 ä¸ªæœˆã€‚
    ã€€ã€€    ç‚¹è¯„ï¼š
        ã€€ã€€æœ¬æ¬¡è‚¡ä»½å›è´­ä½“ç°å…¬å¸å¯¹æœªæ¥å‘å±•å‰æ™¯çš„ä¿¡å¿ƒåŠå¯¹å…¬å¸ä»·å€¼çš„è®¤å¯ï¼Œ æœ‰åˆ©äºç¨³å®šæŠ•èµ„è€…å¯¹å…¬å¸è‚¡ç¥¨é•¿æœŸä»·å€¼çš„é¢„æœŸï¼Œ å›è´­è‚¡ä»½ç”¨äºè‚¡æƒæ¿€åŠ±å¯å……åˆ†è°ƒåŠ¨å…¬å¸è‘£äº‹ã€é«˜ç®¡ã€ä¸­å±‚ç®¡ç†äººå‘˜åŠæ ¸å¿ƒéª¨å¹²çš„ç§¯ææ€§ï¼Œå°†è‚¡ä¸œåˆ©ç›Šã€å…¬å¸åˆ©ç›Šå’Œæ ¸å¿ƒå›¢é˜Ÿåˆ©ç›Šç»“åˆåœ¨ä¸€èµ·ã€‚
        ã€€ã€€äº§å“ç«¯ï¼š è¡£æŸœã€æœ¨é—¨è´¡çŒ®æ–°ä¸šç»©å¢é‡ï¼Œ æ©±è¡£æœ¨ååŒæ•ˆåº”é€æ¸æ˜¾ç°ã€‚1ï¼‰ ä¸æ–­å¼€æ‹“æ–°äº§å“ï¼Œå¤¯å®æ©±æŸœåŸºç¡€ã€‚ å…¬å¸ä½å±…æ©±æŸœè¡Œä¸šç¬¬äºŒï¼Œ æ©±æŸœä¸šåŠ¡å·²å½¢æˆå…­å¤§äº§å“ç³»åˆ—ï¼Œæˆªæ­¢ 2018 å¹´åº•æ©±æŸœä¸šä¸“å–åº— 1487 å®¶ï¼› 2ï¼‰å‘åŠ›å…¨å±‹å®šåˆ¶ï¼Œè¡£æŸœä¸šåŠ¡é«˜é€Ÿå¢é•¿ã€‚ 2018 å¹´ 8 æœˆå…¬å¸ç”±æ•´ä½“å¨æˆ¿ã€å®šåˆ¶è¡£æŸœè½¬å‘å…¨å±‹å®šåˆ¶ï¼Œæˆªæ­¢ 2018 å¹´åº•å®šåˆ¶è¡£æŸœä¸“å–åº— 726 å®¶ï¼Œ IKå…¨å±‹å®šä¸“å–åº— 33 å®¶ã€‚ 3ï¼‰å­µåŒ–æœ¨é—¨ä¸šåŠ¡ï¼Œè´¡çŒ®ä¸šç»©å¢é‡ã€‚é€šè¿‡åˆ‡å…¥æœ¨é—¨å¸‚åœºå¯ä»¥å®ç°æ©±è¡£æœ¨è”åŠ¨ï¼Œæ‹“å±•å…¨å±‹å®šåˆ¶å®¶å±…å“ç±»ï¼Œ æˆªæ­¢ 2018 å¹´åº•å…¬å¸æœ¨é—¨ç»é”€å•† 117 å®¶ï¼Œä¸å…¶ä»–å“ç±»èåˆçš„é—¨åº— 88 å®¶ã€‚
        ã€€ã€€æ¸ é“ç«¯ï¼Œé›¶å”®é—¨åº—æŒç»­æ‹“å±•ï¼Œå¤§å®—å®¢æˆ·ç»“æ„ä¼˜åŒ–ï¼Œ å‡ºå£ä¸šåŠ¡å¿«é€Ÿå‘å±•ã€‚ å…¬å¸è®¡åˆ’ 2019 å¹´æ–°å¼€æ©±æŸœé—¨åº— 150 å®¶ã€ æœ¨é—¨åº— 100 å®¶ã€ å®šåˆ¶è¡£æŸœå’Œå…¨å±‹å®šåˆ¶é—¨åº— 350 å®¶ã€‚å¤§å®—ä¸šåŠ¡æ–¹é¢ï¼Œä¼˜åŒ–å¤§å®—å®¢æˆ·ç»“æ„ï¼Œå¤§å®—å®¢æˆ·å¤šä¸ºç™¾å¼ºå¤´éƒ¨åœ°äº§ã€ä¸»æ¿ä¸Šå¸‚å…¬å¸å’Œåœ°åŒºé¾™å¤´åœ°äº§å…¬å¸ï¼Œ å…¶ä¸­ç™¾å¼ºåœ°äº§å®¢æˆ·å æ¯”å·²è¾¾ 30%ã€‚ 2018 å¹´å…¬å¸å‡ºå£æ”¶å…¥åŒæ¯”+ 87.26%ï¼Œ ï¼Œ åˆ›æ–°æ¾³æ´² IJF è‚¡æƒåˆä½œæ¨¡å¼ï¼Œ æˆç«‹ç¾æ´²å¸‚åœºé¡¹ç›®ç»„ï¼Œ æ¢ç´¢å…¨çƒåŒ–å‘å±•è·¯å¾„ã€‚
        ã€€ã€€æˆ‘ä»¬é¢„è®¡å…¬å¸ 2019-2020 å¹´ EPS åˆ†åˆ«ä¸º 1.41ã€ 1.61 å…ƒï¼Œå¯¹åº” 2019-2020 å¹´ PE åˆ†åˆ«ä¸º 13.56ã€ 11.87 å€ï¼Œ è€ƒè™‘å…¬å¸å…¨å“ç±»ã€å¤šå“ç‰Œå¸ƒå±€å­•è‚²æ–°è¥æ”¶å¢é•¿ç‚¹ï¼Œ ç»´æŒâ€œä¹°å…¥â€è¯„çº§ã€‚
    - **Answer:** 
        >æˆ¿åœ°äº§è°ƒæ§å¯¼è‡´éœ€æ±‚å‡å¼±çš„é£é™©, åŸææ–™æˆæœ¬æ³¢åŠ¨çš„é£é™©ç­‰ã€‚

- **Suggestion:**
    - **Question:** 
        >ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èæŠ•èµ„è€…ï¼Œè¯·æ ¹æ®ç ”æŠ¥ä¸­çš„æŠ•èµ„å»ºè®®ï¼Œç»™å‡ºé€‚åˆè¯¥å…¬å¸çš„æŠ•èµ„è¯„çº§ã€‚ç ”æŠ¥å†…å®¹ï¼š
        æŠ•èµ„è¦ç‚¹ï¼š
        ä¸­å›½ç”µå½±æ˜¯è¦†ç›–å…¨äº§ä¸šé“¾çš„ç”µå½±é¾™å¤´å…¬å¸ï¼Œå®æ§äººæ˜¯ä¸­å½±é›†å›¢
        ä¸­å›½ç”µå½±æ˜¯å›½å†…å”¯ä¸€ä¸€å®¶é›†ç”µå½±åˆ¶ä½œã€å‘è¡Œã€æ”¾æ˜ å’Œå½±è§†æœåŠ¡å…¨äº§ä¸šé“¾è¦†ç›–çš„ä¸Šå¸‚å…¬å¸ï¼Œ ç”±ä¸­å½±é›†å›¢åŠä¸­å›½å›½é™…ç”µè§†ã€å¤®å¹¿ä¼ åª’ã€é•¿å½±é›†å›¢ã€æ±Ÿè‹å¹¿ç”µã€æ­Œåæœ‰çº¿ã€ç”µå¹¿ä¼ åª’ã€ä¸­å›½è”é€š 7 å®¶å…¬å¸å…±åŒå‘èµ·è®¾ç«‹ã€‚ ç›®å‰ä¸­å½±é›†å›¢æ˜¯å…¬å¸çš„æ§è‚¡è‚¡ä¸œå’Œå®é™…æ§åˆ¶äººï¼Œ æ‹¥æœ‰å…¬å¸ 67.36%çš„è‚¡ä»½ï¼› æˆªæ­¢ 2018 å¹´ä¸‰å­£æŠ¥å…¬å¸å‰åå¤§è‚¡ä¸œæŒè‚¡æ¯”ä¾‹ä¸º 76.41%ï¼Œ æŒè‚¡é›†ä¸­åº¦ç›¸å¯¹è¾ƒé«˜ã€‚
        å…¬å¸å‘è¡Œå’Œæ”¾æ˜ ä¸šåŠ¡çš„å®åŠ›å¼ºï¼Œ å…¨äº§ä¸šé“¾ç«äº‰åŠ›çªå‡º
        å…¬å¸çš„ç»è¥ä¸šåŠ¡å…¨é¢è¦†ç›–ç”µå½±äº§ä¸šé“¾çš„ä¸Šä¸­ä¸‹æ¸¸ï¼Œ åˆ†åˆ«åŒ…æ‹¬ä¸Šæ¸¸çš„å½±è§†åˆ¶ç‰‡åˆ¶ä½œä¸šåŠ¡ã€ä¸­æ¸¸çš„ç”µå½±å‘è¡Œè¥é”€ä¸šåŠ¡ä»¥åŠä¸‹æ¸¸çš„ç”µå½±æ”¾æ˜ ã€å½±è§†æœåŠ¡ä¸šåŠ¡ã€‚å…¶ä¸­åœ¨å½±è§†å‘è¡Œè¥é”€ä¸šåŠ¡æ–¹é¢ï¼Œå…¬å¸æ˜¯å›½å†…ä¸¤å®¶è¿›å£ç‰‡å‘è¡Œå•†ä¹‹ä¸€ï¼Œ å…·æœ‰æé«˜çš„æ”¿ç­–å£å’å’Œç«äº‰åŠ›ï¼›åœ¨ç”µå½±æ”¾æ˜ ä¸šåŠ¡æ–¹é¢ï¼Œå…¬å¸æ§è‚¡çš„ä¸­å½±æ•°å­—ã€ä¸­å½±æ˜Ÿç¾ã€ä¸­å½±å—æ–¹æ–°å¹²çº¿ä¸‰å®¶é™¢çº¿æ˜¯å›½å†…é™¢çº¿é¢†åŸŸçš„ TOP10ï¼Œ ç»¼åˆç«äº‰åŠ›çªå‡ºï¼›ç”µå½±æœåŠ¡ä¸šåŠ¡ä¸­ï¼Œå…¬å¸æ‹¥æœ‰ä¸­å›½å·¨å¹•çš„ç ”å‘ã€ç”Ÿäº§å’Œé”€å”®èƒ½åŠ›ï¼Œæ­¤å¤–è¿˜è¿›è¡Œå½±é™¢æ”¾æ˜ è®¾å¤‡çš„é”€å”®ï¼Œå—ç›Šäºè¿‘å¹´æ¥å›½å†…æ–°å»ºå½±é™¢çš„å¿«é€Ÿå¢é•¿ï¼Œå…¬å¸çš„ä¸­å›½å·¨å¹•ç³»ç»Ÿå‘å±•è¿…é€Ÿã€‚å› æ­¤ä»å¸ƒå±€ä¸Šåˆ†æï¼Œ ä¸­å›½ç”µå½±æ— è®ºæ˜¯åœ¨å•é¡¹ä¸šåŠ¡è¿˜æ˜¯æ•´ä½“ä¸šåŠ¡å®åŠ›éƒ½åœ¨å›½å†…å¤„äºç»å¯¹é¾™å¤´çš„åœ°ä½ï¼Œç«äº‰åŠ›å’Œç»è¥å£å’ååˆ†æ˜æ˜¾ã€‚
        ç”µå½±è¡Œä¸šï¼š 2019 å¹´ä¸Šæ˜ å¤§ç‰‡é˜µå®¹è±ªåï¼Œé©±åŠ¨å¸‚åœºå‘å±•
        2018 å¹´ 1-11 æœˆå›½å†…ç”µå½±æ€»ç¥¨æˆ¿æ”¶å…¥ 563.12 äº¿å…ƒï¼Œ åŒæ¯”å¢é•¿ 10.96%ï¼Œ æ•´ä½“è¡¨ç°åŸºæœ¬ç¬¦åˆé¢„æœŸã€‚æˆ‘ä»¬é¢„è®¡ 2019 å¹´å…¨å¹´ç¥¨æˆ¿ç»§ç»­ä¿æŒå¹³ç¨³å¢é•¿ï¼Œ è¿›å£ç‰‡çš„ç›¸å¯¹ç«äº‰ä¼˜åŠ¿æ›´å¼ºï¼Œé€»è¾‘åœ¨äºå¤šéƒ¨ç¾å›½ç§‘å¹»ç”µå½±é™†ç»­ä¸Šæ˜ å°†ä¸ºå›½å†…ç”µå½±ç¥¨æˆ¿å¸‚åœºå½¢æˆæœ‰åŠ›çš„æ”¯æ’‘ï¼›åŒæ—¶ 2019 å¹´æ˜¥èŠ‚æ¡£çš„ä¼—å¤šå›½äº§è´ºå²ç”µå½±ï¼ˆå¦‚æµæµªåœ°çƒç­‰ï¼‰ä¹Ÿå…·å¤‡å¾ˆå¼ºçš„ç«äº‰åŠ›ï¼Œå› æ­¤æ•´ä½“ä¸Šæˆ‘ä»¬è®¤ä¸º 2019 å¹´å›½å†…ç”µå½±ç¥¨æˆ¿å¸‚åœºå°†ç»§ç»­ä¿æŒå¹³ç¨³å¢é€Ÿæ°´å¹³ï¼Œå¢é€Ÿç›¸è¾ƒäº 2018 å¹´å°†æœ‰æ‰€æé«˜ï¼Œ é™¢çº¿å’Œç›¸å…³çˆ†æ¬¾å½±ç‰‡çš„åˆ¶ç‰‡/å‘è¡Œæ–¹å°†æ›´åŠ å—ç›Šã€‚
    - **Answer:** 
        >å¼ºçƒˆæ¨èï¼
        é‰´äºå…¬å¸åœ¨å‘è¡Œã€æ”¾æ˜ é¢†åŸŸå‡æœ‰è¾ƒå¼ºå®åŠ›, ä¸”å¯¹ä¸Šä¸‹æ¸¸äº§ä¸šå‡æœ‰ä¸€å®šçš„è¯è¯­æƒ, æœªæ¥æœ‰æœ›æŒç»­äº«å—é«˜æ¯›åˆ©æ°´å¹³çš„ä¼˜è´¨å†…å®¹äº§å‡ºå¸¦æ¥çš„çº¢åˆ©ã€‚å…¶å¼ºå¤§çš„å‡ºå“èƒ½åŠ›å’Œä¸°å¯Œçš„èµ„æºç§¯ç´¯ä¸ºå…¶æä¾›äº†è‰¯å¥½çš„åˆ›ä½œåŸºç¡€, å¹¶å‡­å€Ÿä¼˜ç§€çš„æ¸ é“æŠŠæ§åŠ›è·å¾—æ›´å¤šä¼˜è´¨å†…å®¹çš„ä¾›åº”ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹, è€ƒè™‘åˆ°ä¸šç»©å¼¹æ€§ç©ºé—´å¤§ï¼ˆå¦‚ã€Šæµæµªåœ°çƒã€‹ç­‰ï¼‰ç­‰å› ç´ , æˆ‘åšä¿¡è¯¥å…¬å¸æœªæ¥çš„å‰æ™¯å€¼å¾—æœŸå¾…ã€‚å½“ç„¶, æŠ•èµ„çš„æ½œåœ¨é£é™©ä»éœ€å…³æ³¨ç›‘ç®¡æ”¿ç­–å’Œç–«æƒ…æƒ…å†µçš„å˜åŒ–ã€‚ä»¥ä¸Šä»…æ˜¯åŸºäºå½“å‰ä¿¡æ¯çš„æ¨ç†é¢„æµ‹, æŠ•èµ„æœ‰é£é™©, å†³ç­–éœ€è°¨æ…ã€‚

- **Event:**
    - **Question:** 
        >ç°åœ¨ä½ ä¼šè¢«æä¾›ä¸€ä»½ä¸­å›½çš„æ–°é—»ï¼šç»™å®šæ–°é—»çš„ä¸€çº§åˆ†ç±»ï¼Œè¯·åœ¨æˆ‘ç»™å®šçš„äºŒçº§æ ‡é¢˜ä¸­è¿›è¡Œé€‰æ‹©ã€‚ä¸€çº§åˆ†ç±»ä¸ºï¼šé‡‘èå¸‚åœºã€‚å¯é€‰çš„äºŒçº§åˆ†ç±»ä¸ºï¼ˆé‡‘èå¸‚åœº, è¡ç”Ÿå“å¸‚åœº, æœŸè´§å¸‚åœº, è´µé‡‘å±, è‚¡ç¥¨å¸‚åœº, Aè‚¡, æ¸¯è‚¡, æˆ¿åœ°äº§, å€ºåˆ¸å¸‚åœº, æ–°ä¸‰æ¿äº¤æ˜“, æµ·å¤–è‚¡ç¥¨å¸‚åœº, å¤–æ±‡å¸‚åœº, äººæ°‘å¸æ±‡ç‡, ç¾å…ƒæ±‡ç‡, å›½é™…é‡‘èå¸‚åœº, æ¬§å…ƒæ±‡ç‡, é“¶è¡Œè´·æ¬¾, è‚¡æƒæŠ•èµ„, æ—¥å…ƒæ±‡ç‡, ETF, å…¶ä»–æ±‡ç‡, ä¿é™©å¸‚åœº, è™šæ‹Ÿè´§å¸, æœŸæƒå¸‚åœº, èµ„äº§è¯åˆ¸åŒ–å¸‚åœº, å¦ç±»å¸‚åœºï¼‰ã€‚æ–°é—»å†…å®¹ï¼šæ¸¯èµ„8æœˆ17æ—¥ï¼Œå‡€ä¹°å…¥58.86ä¸‡è‚¡ï¼Œçº¦335ä¸‡å…ƒï¼Œå¢æŒæ¯”ä¾‹ï¼š0.11%ï¼Œæˆäº¤å æ¯”ï¼š7.591%ï¼ŒæŒè‚¡æ¯”ä¾‹ï¼š0.478%ï¼Œè¿ç»­8ä¸ªäº¤æ˜“æ—¥å¢æŒï¼Œè¿‘5ä¸ªäº¤æ˜“æ—¥æ¯”ä¾‹å˜åŒ–ï¼š118.91%ï¼Œè¿‘22ä¸ªäº¤æ˜“æ—¥æ¯”ä¾‹å˜åŒ–ï¼š383.418%ï¼Œè¿‘66ä¸ªäº¤æ˜“æ—¥æ¯”ä¾‹å˜åŒ–ï¼š292.207%ã€‚
    - **Answer:** 
        >é‡‘èå¸‚åœº, è‚¡ç¥¨å¸‚åœº, Aè‚¡

- **Industry:**
    - **Question:** 
        >ä½ æ˜¯ä¸€åé‡‘èè¡Œä¸šçš„ä¸“å®¶ï¼Œç»™å®šæ–°é—»çš„ä¸€çº§è¡Œä¸šï¼Œè¯·åœ¨æˆ‘ç»™å®šçš„è¡Œä¸šäºŒçº§æ ‡é¢˜ä¸­è¿›è¡Œé€‰æ‹©ã€‚ä¸€çº§åˆ†ç±»ä¸ºï¼šç”µæ°”è®¾å¤‡ã€‚å¯é€‰çš„äºŒçº§åˆ†ç±»ä¸ºï¼ˆç”µæºè®¾å¤‡, é«˜ä½å‹è®¾å¤‡, ç”µæ°”è‡ªåŠ¨åŒ–è®¾å¤‡, ç”µæœºï¼‰ã€‚æ–°é—»å†…å®¹ï¼š    12æœˆ31æ—¥ï¼Œèµ„æœ¬é‚¦è·æ‚‰ï¼Œæ¸¯è‚¡ä¸Šå¸‚å…¬å¸ç¦è±ç‰¹ç»ç’ƒ(06865.HK)äº12æœˆ30æ—¥å‘å¸ƒå…¬å‘Šç§°ï¼Œç¦è±ç‰¹ç»ç’ƒåŠå…¶å…¨èµ„é™„å±å…¬å¸(ä¸‹æ–‡ç®€ç§°â€œå–æ–¹)å·²ä¸æ™¶ç§‘é›†å›¢åŠå…¶å…¨èµ„é™„å±å…¬å¸(ä¸‹æ–‡ç®€ç§°â€œä¹°æ–¹â€)å…³äºé”€å”®å…‰ä¼å‹å»¶ç»ç’ƒäº‹é¡¹ç­¾è®¢ã€Šæˆ˜ç•¥åˆä½œåè®®ã€‹ã€‚æ ¹æ®åè®®ï¼Œä¹°æ–¹åœ¨2021~2023å¹´ä¸‰å¹´å†…å‘å–æ–¹é‡‡è´­å…±è®¡59GW(çº¦3.38äº¿å¹³æ–¹ç±³)ç»„ä»¶ç”¨å…‰ä¼å‹å»¶ç»ç’ƒï¼ŒåˆåŒå±¥è¡ŒæœŸé™è‡ª2021å¹´1æœˆ1æ—¥èµ·è‡³2023å¹´12æœˆ31æ—¥æ­¢ã€‚æŒ‰ç…§å“åˆ›å‘¨æŠ¥2020å¹´12æœˆ24æ—¥å…¬å¸ƒçš„å…‰ä¼ç»ç’ƒå‡ä»·42å…ƒ/å¹³æ–¹(å«ç¨)æµ‹ç®—ï¼Œé¢„ä¼°åˆåŒæ€»é‡‘é¢çº¦141.96äº¿å…ƒäººæ°‘å¸(å«ç¨)ã€‚
        ç¦è±ç‰¹ç»ç’ƒè®¤ä¸ºï¼Œæ­¤æ¬¡åè®®çš„ç­¾è®¢æœ‰åˆ©äºå…¬å¸å…‰ä¼å‹å»¶ç»ç’ƒäº§å“çš„å¸‚åœºæ¨å¹¿ä»¥åŠè¿›ä¸€æ­¥æå‡å…¬å¸ç»è¥ä¸šç»©ã€‚
        æ®æ‚‰ï¼Œç¦è±ç‰¹ç»ç’ƒæ˜¯ä¸€å®¶é›†ç»ç’ƒç ”å‘ã€åˆ¶é€ ã€åŠ å·¥å’Œé”€å”®ä¸ºä¸€ä½“çš„ç»¼åˆæ€§ä¸­å¤§å‹ä¼ä¸šï¼Œåˆ›å»ºäº1998å¹´6æœˆï¼Œé›†å›¢æœ¬éƒ¨ä½äºé•¿ä¸‰è§’ç»æµä¸­å¿ƒæµ™æ±Ÿå˜‰å…´å¸‚ã€‚ç¦è±ç‰¹ç»ç’ƒçš„ä¸»è¦äº§å“æ¶‰åŠå¤ªé˜³èƒ½å…‰ä¼ç»ç’ƒã€ä¼˜è´¨æµ®æ³•ç»ç’ƒã€å·¥ç¨‹ç»ç’ƒã€å®¶å±…ç»ç’ƒå››å¤§é¢†åŸŸï¼Œä»¥åŠå¤ªé˜³èƒ½å…‰ä¼ç”µç«™çš„å»ºè®¾å’ŒçŸ³è‹±å²©çŸ¿å¼€é‡‡ï¼Œå½¢æˆäº†æ¯”è¾ƒå®Œæ•´çš„äº§ä¸šé“¾ã€‚
    - **Answer:** 
        >ç”µæºè®¾å¤‡, é«˜ä½å‹è®¾å¤‡


- **Company:**
    - **Question:** 
        >ä½ æ˜¯ä¸€åå…·æœ‰ä¸°å¯ŒæŠ•é¡¾ç»éªŒçš„é‡‘èç ”ç©¶å‘˜ï¼Œç°åœ¨éœ€è¦ä½ ä¸é‡‘èå¸‚åœºç›¸å…³çš„æ–°é—»å†…å®¹è¿›è¡Œåˆ†æã€‚éœ€æ±‚ï¼šä½ èƒ½å¸®æˆ‘æ€»ç»“å‡ºè¯¥æ–°é—»ä¸­æ¶‰åŠçš„å…¬å¸åç§°å—ï¼Ÿæ–°é—»å†…å®¹ï¼š    æ´‹æ²³å‘å¸ƒæ ¸å¿ƒéª¨å¹²æŒè‚¡è®¡åˆ’ï¼Œæˆäºˆå¯¹è±¡ä¸»è¦åŒ…æ‹¬å…¬å¸é«˜ç®¡ï¼Œæ ¸å¿ƒä¸­å±‚åŠä¸šåŠ¡éª¨å¹²ï¼Œè€ƒæ ¸ç›®æ ‡ä¸º21,22å¹´è¥ä¸šæ”¶å…¥ç›¸æ¯”äºä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥å¢é€Ÿè¶…è¿‡15%ã€‚æˆ‘ä»¬è®¤ä¸ºåœ¨æ¬¡é«˜ç«¯è¡Œä¸šæ¶ˆè´¹å‡çº§ä»¥åŠå…¬å¸å»åº“å­˜æ¨åŠ¨æ¸ é“åˆ©æ¶¦æå‡çš„åŸºç¡€ä¸‹ï¼Œå…¬å¸å®ç°ä»»åŠ¡ç›®æ ‡çš„æ¦‚ç‡è¾ƒé«˜ã€‚ç•¥è°ƒæ•´21-23å¹´EPSä¸º5.57ã€6.76ã€8.23.ç»´æŒ22å¹´37xï¼Œä¸€å¹´ç›®æ ‡ä»·250å…ƒï¼Œç»´æŒâ€œå¼ºçƒˆæ¨è-Aâ€è¯„çº§ã€‚æ¨èé˜…è¯»æˆ‘ä»¬çš„æ·±åº¦æŠ¥å‘Šã€Šæ´‹æ²³æ·±åº¦ï¼šæ”¹é©ä¹‹è·¯ã€‹
        æ´‹æ²³è‚¡ä»½å‘å¸ƒæŒè‚¡è®¡åˆ’ï¼Œä¸šç»©ç¡®å®šæ€§è¿›ä¸€æ­¥æå‡ã€‚æ´‹æ²³è‚¡ä»½å‘å¸ƒç¬¬ä¸€æœŸæ ¸å¿ƒéª¨å¹²æŒè‚¡è®¡åˆ’ï¼Œæˆäºˆå¯¹è±¡ä¸»è¦åŒ…æ‹¬å…¬å¸é«˜ç®¡ä»¥åŠå…¬å¸åŠå…¨èµ„å­å…¬å¸çš„æ ¸å¿ƒä¸­å±‚åŠä¸šåŠ¡éª¨å¹²ã€‚æŒè‚¡è®¡åˆ’å¯¹åº”21,22å¹´ä¸šç»©è€ƒæ ¸æŒ‡æ ‡ä¸ºè¥ä¸šæ”¶å…¥ç›¸æ¯”äºä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥å¢é€Ÿè¶…è¿‡15%ã€‚æœ¬è½®è‚¡æƒæ¿€åŠ±æ€»é‡‘é¢ä¸º10.03äº¿å…ƒï¼Œå¯¹åº”è‚¡æœ¬9,661,310è‚¡ï¼Œå å…¬å¸æ€»è‚¡æœ¬çš„0.64%ï¼Œå¯¹åº”å½“å‰è‚¡ç¥¨å¸‚å€¼18.9äº¿å…ƒã€‚æˆäºˆä»·æ ¼ä¸ºå…¬å¸ä¹‹å‰å›è´­æˆæœ¬,å³103.73å…ƒ/è‚¡ã€‚
        ä½åº“å­˜å’Œæ¸ é“åˆ©æ¶¦æ”¯æ’‘ä¸‹ï¼Œæˆ‘ä»¬åˆ¤æ–­å…¬å¸æœ‰æœ›é¡ºåˆ©å®Œæˆç›®æ ‡ã€‚ä»Šå¹´ä»¥æ¥æ´‹æ²³æŒç»­å»åº“å­˜ï¼Œç›®å‰æ¸ é“åº“å­˜å¤„äºå†å²ä½ä½ï¼Œä¸ŠåŠå¹´ç»ˆç«¯åŠ¨é”€å¢é€Ÿè¶…è¿‡20%ã€‚æˆ‘ä»¬è®¤ä¸ºåœ¨æ¬¡é«˜ç«¯ç™½é…’è¡Œä¸šæ¶ˆè´¹æŒç»­å‡çº§ä»¥åŠå…¬å¸å»åº“å­˜æ¨åŠ¨æ¸ é“ä»·å€¼é“¾ä¸æ–­æå‡çš„åŸºç¡€ä¸‹ï¼Œå…¬å¸é¡ºåˆ©å®ç°ä»»åŠ¡ç›®æ ‡çš„æ¦‚ç‡è¾ƒé«˜ã€‚
        æœ¬è½®è‚¡æƒæ¿€åŠ±è¦†ç›–é¢å¹¿ï¼Œæå‡æ•´ä½“çš„å‘˜å·¥ç§¯ææ€§ã€‚æœ¬è½®æ”¹é©ä¹‹å‰ï¼Œæ´‹æ²³çš„è‚¡æƒæ¿€åŠ±å¯¹è±¡ä¸»è¦é›†ä¸­åœ¨ä¹‹å‰çš„ç®¡ç†å±‚ï¼Œç°ä»»ç®¡ç†å±‚ç¼ºä¹è‚¡æƒæ¿€åŠ±æˆ–ä½“é‡ç›¸å¯¹è¾ƒå°ã€‚åŒæ—¶æœ¬è½®è‚¡æƒæ¿€åŠ±è®¡åˆ’ä¸­ï¼Œæ ¸å¿ƒéª¨å¹²å æ¯”91.72%ï¼Œè¦†ç›–äº†å…¬å¸æ ¸å¿ƒä¸­å±‚åŠä¸šåŠ¡éª¨å¹²ã€‚é•¿æœŸä»¥æ¥æ´‹æ²³ä¾æ‰˜äºæ¸ é“çš„å¼ºæ§åˆ¶åŠ›ä»¥åŠåœ¨å“ç‰Œï¼Œäº§å“ä¸Šçš„ä¸æ–­åˆ›æ–°ï¼ŒæŒç»­æå‡å…¬å¸çš„ç»¼åˆç«äº‰åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºæœ¬è½®è‚¡æƒæ¿€åŠ±æ–¹æ¡ˆæœ‰åŠ©äºå¼ºåŒ–å…¬å¸çš„æ¸ é“æ§åˆ¶åŠ›ä»¥åŠæå‡å…¬å¸ä¸šåŠ¡åˆ›æ–°æ´»åŠ›ï¼ŒæŒç»­æå‡å…¬å¸çš„é•¿æœŸç«äº‰åŠ›ã€‚
        å…¬å¸äº§å“æ”¹é©æŒç»­æ¨è¿›ï¼Œäº§å“åŠ¨é”€æŒç»­åŠ é€Ÿã€‚æˆ‘ä»¬å‰æœŸè°ƒç ”æ˜¾ç¤ºï¼Œå…¬å¸çœå†…å¸‚åœºè€ç‰ˆå¤©ä¹‹è“åº“å­˜å·²ç»åŸºæœ¬æ¶ˆåŒ–å¹²å‡€ï¼Œçœå¤–éƒ¨åˆ†å¸‚åœºå¤©ä¹‹è“ä»æœ‰å°‘é‡åº“å­˜ï¼Œä¸ºäº§å“æ¢ä»£å·²ç»åšå¥½äº†å‡†å¤‡ã€‚æ¢¦ä¹‹è“æ°´æ™¶ç‰ˆé“ºè´§æŒç»­æ¨è¿›ï¼Œçœå†…ç»é”€å•†é¢„è®¡ä»Šå¹´ä¸­ç§‹å›½åº†æœŸé—´å°†è¿æ¥æ”¾é‡ï¼Œç›®å‰æ•´ä½“çš„åº“å­˜æ°´å¹³ç›¸æ¯”äºå¹´åˆå·²ç»æœ‰æ˜æ˜¾ä¸‹é™ï¼Œæ•´ä½“å¤„äºåˆç†åŒºé—´å†…ã€‚è‰æ ¹è°ƒç ”æ˜¾ç¤ºï¼ŒM6+æ‰¹ä»·æŒç»­ç»´æŒåœ¨620å…ƒä»¥ä¸Šï¼Œæ¸ é“åˆ©æ¶¦ç»´æŒåœ¨12%ä»¥ä¸Šï¼Œç»é”€å•†é”€å”®ç§¯ææ€§é«˜ï¼Œæ¸ é“åº“å­˜ç´§å¼ ã€‚
        æŠ•èµ„å»ºè®®ï¼šæ´‹æ²³é«˜ä»·ä½æ®µæ¬¡é«˜ç«¯çš„é¢†å…ˆå¸ƒå±€ä¸æ¸ é“å»åº“å­˜åæŒç»­æ”¹å–„å°†æ‹‰åŠ¨å…¬å¸çš„æˆé•¿ï¼Œç»´æŒâ€œå¼ºçƒˆæ¨è-Aâ€è¯„çº§ã€‚æ´‹æ²³åœ¨600-800å…ƒä»·æ ¼å¸¦å¸ƒå±€é¢†å…ˆå¤§éƒ¨åˆ†ç«äº‰å¯¹æ‰‹ï¼Œæ¢¦6+ä½“é‡åœ¨è¡Œä¸šå†…å…·å¤‡æ˜¾è‘—ä¼˜åŠ¿ï¼Œæœªæ¥å°†ç‡å…ˆæŠ¢å è¯¥ä»·æ ¼å¸¦çš„æˆ˜ç•¥å‘å±•æœºä¼šã€‚ç›®å‰æ¥çœ‹å…¬å¸çœå†…å¤–æ¸ é“æ¢³ç†è¾ƒä¸ºé¡ºç•…ï¼Œæ¸ é“åˆ©æ¶¦å¾—åˆ°æ˜¾è‘—æ¢å¤ï¼Œå…¬å¸2021å¹´å°†å®Œæˆæ°´æ™¶æ¢¦çš„æ¢ä»£ä»¥åŠæ–°ç‰ˆå¤©ä¹‹è“çš„æ¨å‡ºï¼Œæœ‰æœ›åœ¨2022å¹´è¿›ä¸€æ­¥åŠ©æ¨å…¬å¸çš„æˆé•¿ã€‚ç•¥è°ƒæ•´21-23å¹´EPSä¸º5.57ã€6.76ã€8.23.ç»´æŒ22å¹´37xï¼Œä¸€å¹´ç›®æ ‡ä»·250å…ƒï¼Œç»´æŒâ€œå¼ºçƒˆæ¨è-Aâ€è¯„çº§ã€‚
        é£é™©æç¤ºï¼šè¡Œä¸šéœ€æ±‚å›è½ï¼Œçœå†…ç«äº‰åŠ å‰§ï¼Œçœå¤–æ‹“å±•ä¸åŠé¢„æœŸã€‚

    - **Answer:** 
        >æ´‹æ²³è‚¡ä»½

- **Product:**
    - **Question:** 
        >ä½ æ˜¯ä¸€åå…³æ³¨é‡‘èå¸‚åœºçš„æŠ•èµ„è€…ï¼Œè¯·åˆ†æè¿™ç¯‡æ–°é—»ã€‚éº»çƒ¦æ‚¨æ‰¾å‡ºè¯¥æ–°é—»ä¸­æ¶‰åŠåˆ°çš„ç›¸å…³äº§å“ã€‚æ–°é—»å†…å®¹ï¼š    æ–¹æ­£ä¸­æœŸç ”ç©¶å‘˜ å²å®¶äº®ã€è¡Œæƒ…å¤ç›˜ã€‘è™½ç„¶æœ‰è‰²æ¿å—åå¼±ï¼Œç„¶æ²ªé”¡åœ¨ç°è´§ç´§ç¼ºå› ç´ æ¨åŠ¨ä¸‹é€†åŠ¿ä¸Šæ¶¨ï¼Œè¡¨ç°ä¸€æç‹¬ç§€ï¼Œç»§ç»­å¤§æ¶¨ï¼Œ09åˆçº¦å°¾ç›˜æ¶¨2.71%è‡³23.67ä¸‡å…ƒï¼Œç»“ç®—ä»·ä¸º23.29ä¸‡å…ƒã€‚å½“å‰ï¼Œé”¡ç°è´§ç«¯ç¼ºè´§ä¾æ—§ï¼Œå½“å‰ä¸ŠæœŸæ‰€ä»“å•ç”šè‡³éš¾ä»¥è¦†ç›–08åˆçº¦å¾…äº¤å‰²é‡ï¼Œå¹¶ä¸”äº‘é”¡å·²ç»å¤äº§äº†ä¹Ÿæ²¡èƒ½ç¼“è§£ç°è´§ç´§ç¼ºçš„ç°çŠ¶ï¼Œæ‰€ä»¥å¼ºä¾›éœ€æ”¯æ’‘ä¸‹é”¡ä»·å¼ºåŠ¿è¡Œæƒ…ä¾æ—§ã€‚ä¼¦é”¡æ–¹é¢ï¼Œæˆªæ­¢17ï¼š00ï¼ŒLMEä¸‰ä¸ªæœˆæœŸé”¡ç°æŠ¥35165ç¾å…ƒ/å¨ï¼Œåˆ›å†å²æ–°é«˜ï¼Œæ•´ä½“å¼ºåŠ¿è¡Œæƒ…ä»åœ¨æŒç»­ï¼Œå¯¹æ²ªé”¡çš„æœ‰æ•ˆæ”¯æ’‘ä¾ç„¶å­˜åœ¨ã€‚ç»¼åˆè€ƒè™‘æ±‡ç‡ã€å¢å€¼ç¨ç‡å’Œæ¸¯æ‚è´¹ç­‰å› ç´ ï¼ˆæ ¹æ®äº†è§£ï¼Œæš‚æ—¶æ— 8%çš„å…³ç¨ï¼‰ï¼Œå¯¹é”¡ä»·å†…å¤–ç›˜ä»·æ ¼è¿›è¡Œæµ‹ç®—ï¼Œ3.45ä¸‡çš„ä¼¦é”¡å¯¹åº”çš„åˆ°å²¸ä»·æ ¼ä¸º25.34ä¸‡å…ƒï¼Œ3.5ä¸‡çš„ä¼¦é”¡å¯¹åº”çš„åˆ°å²¸ä»·æ ¼ä¸º26.44ä¸‡å…ƒï¼›å³ä½¿åŠ å…¥è¿è´¹çš„è€ƒé‡ï¼Œä¼¦é”¡ä»·æ ¼ä¾ç„¶é«˜äºæ²ªé”¡ä»·æ ¼ã€‚
        é‡è¦èµ„è®¯ã€‘åŸºæœ¬é¢æ–¹é¢ä¾ç„¶åˆ©å¥½é”¡ä»·ï¼šåº“å­˜æ–¹é¢ï¼Œä¸ŠæœŸæ‰€æŒ‡å®šäº¤å‰²ä»“åº“ä»“å•ç»§ç»­ä¸‹è·Œ56å¨è‡³1514å¨ï¼Œæ•´ä½“å»åº“è¶‹åŠ¿ä¾æ—§ï¼Œå¹¶ä¸”ç°è´§ç«¯ä¾ç„¶åç´§ï¼Œæ¶ˆè´¹æ—ºå­£å»åº“è¶‹åŠ¿ä»å°†ä¼šæŒç»­ï¼›ä¼¦é”¡åº“å­˜æ–¹é¢ï¼Œä¼¦é”¡åº“å­˜ç»´æŒåœ¨2245å¨å·¦å³ï¼Œå½“å‰åº“å­˜ä¾ç„¶å¤„äºä½ä½ã€‚é”¡é”­äº§é‡æ–¹é¢ï¼Œå—äº‘é”¡å†¶ç‚¼ç³»ç»Ÿå‡çº§æ”¹é€ ä»¥åŠåé”¡å—å½“åœ°é™ç”µæ”¿ç­–å½±å“ï¼Œå†…è’™ã€äº‘å—åœ°åŒºéƒ¨åˆ†ä¼ä¸šä»å› åŸæ–™çŸ­ç¼ºåŠåŠ å·¥è´¹ä½è¿·é—®é¢˜ç»´æŒä½é‡ç”Ÿäº§ï¼Œ7æœˆå›½å†…ç²¾é”¡æ€»äº§é‡ä¸‹é™æ˜æ˜¾ï¼Œåå¸‚è¿›å£å’Œå›½å†…åŸææ–™çš„ä¾›åº”ä¾ç„¶ä¼šåç´§ã€‚å—é”¡é”­å‡ºå£ä¸‹é™å½±å“ï¼Œé”¡é”­è¡¨è§‚æ¶ˆè´¹é‡ä½ä½å›å‡ï¼›å¹¶ä¸”6æœˆé”¡ç²¾çŸ¿è¿›å£å‡ºç°å¤§å¹…å›å‡äº¦ä½¿å¾—é”¡ç²¾çŸ¿è¡¨è§‚æ¶ˆè´¹é‡å‡ºç°å¤§å¹…å›å‡ï¼Œæ”¶å¤5æœˆè·Œå¹…ï¼›ç„¶è€Œå³ä½¿å¦‚æ­¤ï¼Œå›½å†…é”¡ç°è´§å¸‚åœºè´§æºä¾ç„¶ç´§å¼ ï¼Œé”¡é”­é‡‡è´­éš¾åº¦è¾ƒå¤§ï¼Œè¯´æ˜é”¡é”­çš„éœ€æ±‚äº¦æ˜¯å¼ºåŠ²ï¼Œè¿™å‡æ”¯æ’‘äº†é”¡ä»·ã€‚äº‘é”¡å·²ç»å¤äº§ï¼Œä½†æ˜¯ä¾ç„¶æ²¡æœ‰ç¼“è§£é”¡ç°è´§çš„ç´§ç¼ºç¨‹åº¦ã€‚
        ã€äº¤æ˜“ç­–ç•¥ã€‘åŸºæœ¬é¢å’Œå¤–ç›˜è¡¨ç°ç»§ç»­æ”¯æ’‘ç€å†…ç›˜é”¡ä»·ï¼Œæ²ªé”¡å»åº“è¶‹åŠ¿æŒç»­ï¼Œæ•…æ²ªé”¡ä¾›éœ€åŸºæœ¬é¢ä¾ç„¶è¾ƒå¼ºï¼Œç‰¹åˆ«æ˜¯ç°è´§ç«¯ç´§ç¼ºä¾æ—§ï¼Œé”¡çš„æ•´ä½“å¼ºåŠ¿è¡Œæƒ…ä»åœ¨æŒç»­ã€‚å½“å‰è¿›å…¥é«˜ä½éœ‡è¡è¡Œæƒ…ï¼Œä½†æ˜¯ä¾ç„¶ä¸å»ºè®®åšç©ºï¼Œç‰¹åˆ«æ˜¯ä¼¦é”¡ä¾ç„¶å¼ºäºæ²ªé”¡å’ŒåŸºæœ¬é¢æ²¡æœ‰å‘ç”Ÿå˜åŒ–èƒŒæ™¯ä¸‹ï¼Œä¾ç„¶ç»´æŒé€¢ä½åšå¤šæ€è·¯ï¼Œä¸Šæ–¹ä»æœ‰åˆ·æ–°å†å²æ–°é«˜çš„å¯èƒ½ï¼ˆæ¶¨è‡³24ä¸‡ä¸Šæ–¹åï¼Œä¸Šæ¢25ä¸‡é«˜ä½çš„å¯èƒ½æ€§ä»å­˜ï¼‰ï¼ŒçŸ­æœŸå†…ä¸‹æ–¹æ”¯æ’‘ä½ä¸º22.6ä¸‡çš„å’Œ22.3ä¸‡çš„å‰ä½ä½ç½®ï¼›å›è°ƒåˆ™æ˜¯åšå¤šæœºä¼šã€‚
    - **Answer:** 
        >é”¡, æ²ªé”¡, ä¼¦é”¡, äº‘é”¡

- **Exam:**
    - **Question:** 
        >å‘½é¢˜ï¼šæŒ‰ç…§å•ä½ä¸šåŠ¡æ•°é‡åˆ¶å®šä¸€ä¸ªå›ºå®šä¸å˜çš„èµ„è´¹æ¯”ç‡ï¼Œæ ¹æ®ç”¨æˆ·ä½¿ç”¨çš„é€šä¿¡ä¸šåŠ¡æ•°é‡çš„å¤šå°‘æ”¶å–é€šä¿¡è´¹ç”¨çš„ä¸€ç§èµ„è´¹æ¨¡å¼æ˜¯å®šé¢èµ„è´¹ã€‚ åˆ†æè§£é‡Šï¼šæœ¬é¢˜è€ƒæŸ¥çº¿æ€§èµ„è´¹ã€‚çº¿æ€§ä»é‡èµ„è´¹æŒ‡çš„æ˜¯æŒ‰ç…§å•ä½ä¸šåŠ¡æ•°é‡åˆ¶å®šä¸€ä¸ªå›ºå®šä¸å˜çš„èµ„è´¹æ¯”ç‡ï¼Œæ ¹æ®ç”¨æˆ·ä½¿ç”¨çš„é€šä¿¡ä¸šåŠ¡æ•°é‡çš„å¤šå°‘æ”¶å–é€šä¿¡è´¹ç”¨çš„ä¸€ç§èµ„è´¹æ¨¡å¼ã€‚æ ¹æ®ä»¥ä¸Šåˆ†æè§£é‡Šï¼Œä½ è®¤ä¸ºä¸Šè¿°å‘½é¢˜æ˜¯å¦æ­£ç¡®ï¼Ÿ
    - **Answer:** 
        >é”™è¯¯ã€‚

- **Stock:**
    - **Question:** 
        >ä½ æ˜¯ä¸€ä¸ªé‡åŒ–äº¤æ˜“ç ”ç©¶å‘˜ã€‚è¯·ä½ æ ¹æ®æä¾›çš„èƒŒæ™¯ä¿¡æ¯ï¼Œæ¨æµ‹è´µå·èŒ…å°æœªæ¥äº”å¤©çš„è‚¡ä»·å˜åŒ–è¶‹åŠ¿ã€‚å½“å‰è´µå·èŒ…å°çš„å‰åå¤©çš„å†å²æ”¶ç›˜ä»·ä¸º[1802.59, 1791.0, 1788.0, 1774.0, 1816.3, 1824.98, 1834.97, 1851.33, 1856.0, 1847.0,]ã€‚ç›¸å…³çƒ­ç‚¹æ–°é—»ä¿¡æ¯ï¼šè¿‘æœŸè´µå·èŒ…å°ä¸ç‘å¹¸å’–å•¡è·¨ç•Œå³å°†æ¨å‡ºçš„èŒ…å°æ‹¿é“è¿…é€Ÿå‡ºåœˆï¼Œé¢„è®¡é”€é‡çªç ´å•æ—¥500ä¸‡ã€‚ç›¸å…³ç¤¾äº¤åª’ä½“çƒ­ç‚¹è¯„è®ºï¼šç‘å¹¸xèŒ…å°ã€Œé…±é¦™æ‹¿é“ã€çˆ†çº¢ï¼›8æœˆ31æ—¥æ¶ˆæ¯ï¼Œæƒ³å¿…å¾ˆå¤š80/90åéƒ½å¬è¿‡ã€Šç¾é…’åŠ å’–å•¡ã€‹è¿™é¦–è€æ­Œï¼Œæ˜¯çš„ï¼Œå®ƒçœŸçš„è¦æ¥äº†ã€‚è¿‘æ—¥ï¼Œç½‘ä¼ èŒ…å°å°†ä¸ç‘å¹¸å°†æ¨å‡ºè”åæ¬¾å’–å•¡ï¼Œç›®å‰æ¯å¥—ã€çº¸è¢‹çš„å‰§é€å›¾å·²æµå‡ºï¼Œæ›å…‰çš„æ¯å¥—å’Œçº¸è¢‹ä¸Šéƒ½æœ‰éå¸¸æ˜æ˜¾çš„èŒ…å°é£æ ¼ï¼Œä¸Šé¢è¿˜æ ‡æ³¨ç€ï¼šâ€œä¸­å›½äººçš„ç¬¬ä¸€æ¯é…±é¦™æ‹¿é“â€å’Œâ€œç¾é…’åŠ å’–å•¡ï¼Œå°±çˆ±è¿™ä¸€æ¯â€ï¼›æ®è´¢è”ç¤¾æŠ¥é“ï¼Œæœ‰æ¥è¿‘èŒ…å°çš„äººå£«é€éœ²ï¼Œæ­¤æ¬¡è”åå’–å•¡æ˜¯çŸ­æœŸæ€§çš„ï¼Œè€Œä¸”æœ‰é™é‡ã€‚æ®æ­¤å¯ä»¥çŒœæµ‹ï¼Œè¿™æ¬¾æ‹¿é“æ€•æ˜¯åˆè¦å—åˆ°å¸‚åœºçƒ­ç‚’äº†ã€‚ 
    - **Answer:** 
        >æ ¹æ®å†å²è‚¡ä»·ã€çƒ­ç‚¹æ–°é—»å’Œç¤¾åª’è¯„è®ºï¼Œè´µå·èŒ…å°æœ€è¿‘ä¸ç‘å¹¸å’–å•¡åˆä½œæ¨å‡ºèŒ…å°æ‹¿é“ï¼Œè¿™å°†å¯¼è‡´é”€é‡æ€¥å‰§å¢é•¿ä»¥åŠä¼ä¸šçŸ¥ååº¦è¿›ä¸€æ­¥æå‡ï¼Œå¹¶æœ‰åŠ©äºæŒ–æ˜æ–°çš„å®¢æˆ·ç¾¤ä½“ï¼Œå¢å¼ºäº§å“ç²˜æ€§ã€‚è¿™ç§ç§¯æçš„å¸‚åœºååº”å¯èƒ½å¯¹è´µå·èŒ…å°çš„è‚¡ä»·äº§ç”Ÿæ­£é¢å½±å“ã€‚ç„¶è€Œï¼Œè‚¡ä»·çš„èµ°åŠ¿è¿˜ä¼šå—åˆ°å…¶ä»–å› ç´ çš„å½±å“ï¼Œå¦‚æ•´ä½“å¸‚åœºæƒ…å†µã€å…¬å¸è´¢åŠ¡è¡¨ç°ã€è¡Œä¸šç«äº‰ç­‰ã€‚
        æ•´ä½“æ¥çœ‹ï¼Œæœªæ¥äº”å¤©è´µå·èŒ…å°çš„è‚¡ä»·å¯èƒ½ä¼šå‡ºç°1%å·¦å³çš„å¢å¹…ã€‚

# Benchmark

We release the [CFBenchmark-Basic](https://github.com/TongjiFinLab/CFBenchmark), a chinese finance benchmark, including 3917 financial texts spanning three aspects and eight tasks, for the evaluation of the financial text processing capability of LLMs in Chinese financial market.

CFBenchmark-Basic utilize two types of metrics to evaluate the performance of LLMs in the financial domain on our CFBenchmark-Basic. 
For recognition and classification tasks, we employe the **F1 score** as the evaluation metric, which balance the precision and recall.  
For the generation tasks, we utilize **cosine similarity** between the vectored representation of ground truth and generated answer as measure the generation ability.  Since there are usually different expressions with the similar meaning in our generation tasks, simply employing Rough-Score or BULE-socre is not reasonable. Specifically, the **bge-zh-v1.5** is assigned as the oracle model to generate the sentence embedding. We calculate evaluation scores for each sub-task individually and provide the average score for each category.

The best scores of LLMs (considering zero-shot and few-shot),as well as which of our model,  are demonstrated below:

| Model              | Size | Company | Product | R.Avg   | Industry | Event   | Sentiment | C.Avg   | Summary | Risk    | Suggestion | G.Avg   | Avg     |
| ------------------ | ---- | ------- | ------- | -----   | -------- | -----   | --------- | -----   | ------- | -----   | ---------- | -----   | -----   |
| ChatGPT            | 20B  | 0.797   | 0.198   | 0.498   | 0.453    | 0.458   | 0.425     | 0.455   | 0.593   | 0.541   | 0.771      | 0.635   | 0.529   |
| ERNIE-Bot          | 260B | 0.807   | 0.300   | 0.533   | 0.408    | 0.350   | 0.186     | 0.315   | 0.715   | 0.590   | 0.716      | 0.673   | 0.507   |
| ERNIE-Bot-4        | -    | 0.819   | 0.417   | 0.618   | 0.418    | 0.358   | 0.375     | 0.384   | 0.721   | 0.629   | 0.718      | 0.689   | 0.564   |
| Falcon-7B          | 7B   | 0.671   | 0.168   | 0.420   | 0.169    | 0.132   | 0.250     | 0.184   | 0.302   | 0.301   | 0.246      | 0.283   | 0.296   |
| Falcon-7B-chat     | 7B   | 0.582   | 0.046   | 0.314   | 0.112    | 0.142   | 0.153     | 0.135   | 0.307   | 0.299   | 0.258      | 0.288   | 0.246   |
| bloomz-7B1         | 7B   | 0.765   | 0.166   | 0.465   | 0.252    | 0.154   | 0.394     | 0.267   | 0.451   | 0.371   | 0.462      | 0.428   | 0.387   |
| bloomz-7Bt1-mt     | 7B   | 0.751   | 0.157   | 0.454   | 0.087    | 0.182   | 0.380     | 0.216   | 0.425   | 0.379   | 0.396      | 0.400   | 0.357   |
| Qwen-7B            | 7B   | 0.780   | 0.357   | 0.569   | 0.480    | 0.335   | 0.379     | 0.398   | 0.750   | 0.505   | 0.713      | 0.656   | 0.541   |
| Qwen-Chat-7B       | 7B   | 0.763   | 0.360   | 0.562   | 0.400    | 0.367   | 0.265     | 0.344   | 0.548   | 0.307   | 0.379      | 0.411   | 0.439   |
| Qwen-14B           | 14B  | 0.805   | 0.421   | 0.613   | 0.481    | 0.350   | 0.385     | 0.405   | 0.754   | 0.608   | 0.717      | 0.693   | 0.570   |
| Qwen-Chat-14B      | 14B  | 0.814   | 0.442   | 0.628   | 0.382    | 0.400   | 0.350     | 0.377   | 0.732   | 0.478   | 0.736      | 0.649   | 0.551   |
| ChatGLM2-6B        | 6B   | 0.747   | 0.313   | 0.530   | 0.285    | 0.300   | 0.357     | 0.314   | 0.657   | 0.454   | 0.671      | 0.594   | 0.479   |
| Baichuan2-7B-Base  | 7B   | 0.672   | 0.340   | 0.506   | 0.342    | 0.490   | 0.480     | 0.437   | 0.739   | 0.619   | 0.751      | 0.703   | 0.549   |
| Baichuan2-7B-Chat  | 7B   | 0.757   | 0.402   | 0.579   | 0.425    | 0.475   | 0.323     | 0.408   | 0.725   | 0.648   | 0.732      | 0.702   | 0.563   |
| Baichuan2-13B-Base | 13B  | 0.781   | 0.330   | 0.555   | 0.436    | 0.496   | 0.477     | 0.470   | 0.725   | 0.503   | 0.747      | 0.658   | 0.561   |
| Baichuan2-13B-Chat | 13B  | 0.797   | 0.314   | 0.556   | 0.472    | 0.507   | 0.387     | 0.455   | 0.739   | 0.634   | 0.746      | 0.706   | 0.572   |
| InternLM-7B        | 7B   | 0.612   | 0.233   | 0.423   | 0.266    | 0.311   | 0.328     | 0.302   | 0.378   | 0.336   | 0.379      | 0.364   | 0.363   |
| InternLM-7B-Chat   | 7B   | 0.632   | 0.261   | 0.447   | 0.272    | 0.364   | 0.399     | 0.345   | 0.363   | 0.270   | 0.353      | 0.329   | 0.374   |
| InternLM-20B       | 20B  | 0.809   | 0.358   | 0.583   | 0.500    | 0.427   | 0.417     | 0.448   | 0.706   | 0.653   | 0.728      | 0.695   | 0.575   |
| InternLM-20B-Chat  | 20B  | 0.488   | 0.362   | 0.425   | 0.323    | 0.327   | 0.370     | 0.340   | 0.706   | 0.578   | 0.762      | 0.662   | 0.476   |
| CFGPT1-stf-LoRA    | 7B   | 0.820   | 0.414   | 0.617   | 0.569    | 0.729   | 0.769     | 0.689   | 0.745   | 0.584   | 0.609      | 0.646   | 0.650   |
| CFGPT1-sft-Full    | 7B   |**0.836**|**0.476**|**0.656**|**0.700** |**0.808**|**0.829**  |**0.779**|**0.798**|**0.669**|**0.808**   |**0.758**|**0.731**|

More details can be found in [CFBenchmark-Basic](https://github.com/TongjiFinLab/CFBenchmark)

# Acknowledgements

CFGPT has referred to the following open-source projects. We want to express our gratitude and respect to the researchers of the projects.

- InternLM: https://github.com/InternLM/InternLM
- Firefly: https://github.com/yangjianxin1/Firefly
- FinGPT: https://github.com/AI4Finance-Foundation/FinGPT

# To-Do List
- [ ] Series of applications with CFGPT.
- [ ] Constructing more comprehensive training tasks and their corresponding databases.
- [ ] Continued improvement of the capabilities of CFGPT in more complex financial tasks.

# License
CFGPT is a research preview intended for non-commercial use only, subject to the model License of InternLM and the Terms of Use of the data generated by OpenAI. Please contact us if you find any potential violations. The code is released under the Apache License 2.0. 

# Citation
If you use the code or data of [**CFGPT**](https://arxiv.org/abs/2309.10654), please declare the reference with the following:

```
@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```