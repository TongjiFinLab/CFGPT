<div style="text-align:center">
<!-- <img src="https://big-cheng.com/k2/k2.png" alt="k2-logo" width="200"/> -->
<h2>ğŸ“ˆ CFGPT: Chinese Financial Assistant with Large Language Model</h2>
</div>

<a href='https://arxiv.org/abs/2309.10654'><img src='https://img.shields.io/badge/Paper-ArXiv-C71585'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(pt)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20LoRA)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20Full)-red'></a> 

[English](README.md) | ç®€ä½“ä¸­æ–‡

# ç®€ä»‹

**CFGPT**æ˜¯ä¸€ä¸ªå¼€æºçš„è¯­è¨€æ¨¡å‹ï¼Œé¦–å…ˆé€šè¿‡åœ¨æ”¶é›†å’Œæ¸…ç†çš„ä¸­å›½é‡‘èæ–‡æœ¬æ•°æ®ï¼ˆCFData-ptï¼‰ä¸Šè¿›è¡Œç»§ç»­é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬é‡‘èé¢†åŸŸç‰¹å®šæ•°æ®ï¼ˆå…¬å‘Šã€é‡‘èæ–‡ç« ã€é‡‘èè€ƒè¯•ã€é‡‘èæ–°é—»ã€é‡‘èç ”ç©¶è®ºæ–‡ï¼‰å’Œé€šç”¨æ•°æ®ï¼ˆç»´åŸºç™¾ç§‘ï¼‰ï¼Œç„¶åä½¿ç”¨çŸ¥è¯†å¯†é›†çš„æŒ‡å¯¼è°ƒæ•´æ•°æ®ï¼ˆCFData-sftï¼‰è¿›è¡Œå¾®è°ƒã€‚
æˆ‘ä»¬ä½¿ç”¨CFBenchmark-Basicè¿›è¡Œåˆæ­¥è¯„ä¼°ã€‚ä¸å‡ ä¸ªå…·æœ‰ç›¸ä¼¼å‚æ•°çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCFGPTåœ¨è¯†åˆ«ï¼Œåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚

- æˆ‘ä»¬å¼€å‘äº†CFGPT2 (13B)ï¼Œè¿™ä¸ä»…æ˜¯ä¸€ä¸ªåŠŸèƒ½æ›´åŠ å¼ºå¤§çš„ä¸­æ–‡é‡‘èå¤§æ¨¡å‹ï¼ŒåŒæ—¶è¿˜èåˆäº†æ£€ç´¢å¢å¼ºæ¨¡å—ï¼Œäº‹å®æ£€æµ‹æ¨¡å—ï¼Œåˆè§„æ£€æŸ¥æ¨¡å—å’Œé£é™©ç›‘æµ‹æ¨¡å—ï¼Œåœ¨æå‡é‡‘èå¤§æ¨¡å‹æœåŠ¡çš„å®æ—¶æ€§ä¸å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆç›‘æµ‹ä¸ç®¡æ§é‡‘èé£é™©ã€‚
- æˆ‘ä»¬å°†CFGPT1 (7B) åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†å‘å¸ƒï¼š
    - [Pretrained Model](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B): åœ¨ä¸­å›½é‡‘èæ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œè¿›ä¸€æ­¥é¢„è®­ç»ƒä¸”ç¬¦åˆInternLMæ¨¡å‹è®¸å¯çš„å®Œæ•´æ¨¡å‹æƒé‡ã€‚
    - [Supervised Finetuned Model (Lora)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA): åŸºäºæˆ‘ä»¬ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹çš„ç”±PEFTï¼ˆLoRAï¼‰è®­ç»ƒçš„é€‚é…å™¨æ¨¡å‹æƒé‡ã€‚
    - [Supervised Finetuned Model (Full)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full): åŸºäºæˆ‘ä»¬ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹çš„è¿›ä¸€æ­¥å…¨å‚æ•°å¾®è°ƒçš„å®Œæ•´æ¨¡å‹è®­ç»ƒæƒé‡ã€‚

- æˆ‘ä»¬è¿˜å‘å¸ƒäº†[CFBenchmark](https://github.com/TongjiFinLab/CFBenchmark)ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡é‡‘èåŸºå‡†æµ‹è¯•ã€‚åŸºç¡€ç‰ˆæœ¬çš„CFBenchmarkåŒ…æ‹¬3917ä¸ªé‡‘èæ–‡æœ¬ï¼Œæ¶µç›–ä¸‰ä¸ªæ–¹é¢å’Œå…«ä¸ªä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°ä¸­æ–‡é‡‘èå¸‚åœºä¸­LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é‡‘èæ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚

- æˆ‘ä»¬è¿˜å‘å¸ƒäº†CFGPTçš„è¿›ä¸€æ­¥é¢„è®­ç»ƒå’ŒæŒ‡å¯¼å¾®è°ƒçš„ä»£ç ã€‚

- æˆ‘ä»¬è¿˜æä¾›äº†CFData-sftçš„ç›¸å…³ç¤ºä¾‹æ•°æ®ï¼Œä»¥æ–¹ä¾¿ä½¿ç”¨è€…ç†è§£æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹

***ä»¥ä¸‹æ˜¯è®­ç»ƒ CFGPT çš„æµç¨‹æ¦‚è§ˆå›¾ï¼š***

<div align="center">
<img align="center" src=./figs/CFGPT-Training.svg width="100%"/>
</div>

# ç›®å½•

- [å¿«é€Ÿä½¿ç”¨](#å¿«é€Ÿä½¿ç”¨)
- [å…¸å‹ä½¿ç”¨æ¡ˆä¾‹](#å…¸å‹ä½¿ç”¨æ¡ˆä¾‹)
- [æ•°æ®é›†](#æ•°æ®é›†)
- [ä»£ç ](#ä»£ç )
- [è¯„æµ‹](#è¯„æµ‹)
- [è‡´è°¢](#è‡´è°¢)
- [æœªæ¥å·¥ä½œ](#æœªæ¥å·¥ä½œ)
- [ä½¿ç”¨è®¸å¯](#ä½¿ç”¨è®¸å¯)
- [å¼•ç”¨](#å¼•ç”¨)

# å¿«é€Ÿä½¿ç”¨

**1. å‡†å¤‡ä»£ç å’Œç¯å¢ƒ**

å…‹éš†æˆ‘ä»¬çš„ä»“åº“ï¼Œåˆ›å»ºä¸€ä¸ªPythonç¯å¢ƒï¼Œå¹¶é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¿€æ´»å®ƒï¼š
```bash
git clone https://github.com/TongjiFinLab/CFGPT.git
cd CFGPT
conda create -n env_name python=3.10   
source activate env_name 
pip install -r requirements.txt
```

**2. å‡†å¤‡é¢„è®­ç»ƒçš„ CFGPT1**

CFGPT1ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†InternLM-7Båœ¨æˆ‘ä»¬çš„CFData-ptä¸Šç»§ç»­é¢„è®­ç»ƒï¼Œä¸€ä¸ªLoRAæ¨¡å‹ï¼ˆé€šè¿‡PEFTåœ¨æˆ‘ä»¬çš„CFData-sftä¸Šè®­ç»ƒï¼‰ï¼Œä»¥åŠåŸºäºç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ç›‘ç£å¾®è°ƒè®­ç»ƒçš„å…¨ç²¾è°ƒæ¨¡å‹ã€‚

|Pretrain model|Adapter model|Full SFT Model|
|:-:|:-:|:-:|
 [CFGPT1-pt-7B](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B)|[CFGPT1-sft-7B-LoRA](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA)|[CFGPT1-sft-7B-Full](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full)|

**3. ä½¿ç”¨ CFGPT1-sft-7B-LoRA**

```python
from transformers import AutoModel, AutoTokenizer
from peft import PeftModel
base_model = 'TongjiFinLab/CFGPT1-pt-7B'
lora_weights = 'TongjiFinLab/CFGPT1-sft-7B-LoRA'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = PeftModel.from_pretrained(
    model,
    lora_weights,
    device_map=device_map,
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

**4. ä½¿ç”¨ CFGPT1-sft-7B-Full**

```python
from transformers import AutoModel, AutoTokenizer
base_model = 'TongjiFinLab/CFGPT1-sft-7B-Full'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

- **æ›´å¤šä½¿ç”¨ç»†èŠ‚åœ¨ `./code/test`**

# å…¸å‹ä½¿ç”¨æ¡ˆä¾‹

- [CFGPT-v2-13B é“¶è¡Œä¸šåœºæ™¯ä½¿ç”¨æ¡ˆä¾‹](cases/case_bank.md)
- [CFGPT-v2-13B ç®—æ³•äº¤æ˜“åœºæ™¯ä½¿ç”¨æ¡ˆä¾‹](cases/case_algor_trading.md)
- [CFGPT-v2-13B ç¬¬ä¸€äº§ä¸šåœºæ™¯ä½¿ç”¨æ¡ˆä¾‹](cases/case_primary_industry.md)
- [CFGPT-v1-7B å…¸å‹æ•°æ®æ¡ˆä¾‹ä»‹ç»](cases/case_CFGPTv1.md)

# æ•°æ®é›†

åœ¨è¿™ä¸ªå­˜å‚¨åº“ä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†CFDataçš„æ ·æœ¬ï¼š
- CFDataï¼š`./data`

    CFDataåŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒæ•°æ®é›†ï¼ˆCFData-ptï¼‰å’Œä¸€ä¸ªç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼ˆCFData-sftï¼‰ï¼Œå…¶ä¸­é¢„è®­ç»ƒæ•°æ®é›†æ±‡é›†äº†ä¸­å›½é‡‘èæ•°æ®å’Œåˆ†æï¼Œä»¥åŠä¸€ä¸ªè¾ƒå°çš„é€šç”¨æ–‡æœ¬å­é›†ï¼Œæ€»å…±æœ‰5.84äº¿ä¸ªæ–‡æ¡£å’Œ1410äº¿ä¸ªtokenï¼Œç›‘ç£å¾®è°ƒæ•°æ®é›†ä¸“ä¸ºå…­ç§ä¸åŒçš„é‡‘èä»»åŠ¡é‡èº«å®šåˆ¶ï¼Œæ¶µç›–äº†é‡‘èåˆ†æå’Œå†³ç­–åˆ¶å®šçš„å„ä¸ªæ–¹é¢ï¼Œå…±æœ‰150ä¸‡ä¸ªæŒ‡ä»¤å¯¹å’Œ150äº¿ä¸ªtokenã€‚


## ç»§ç»­é¢„è®­ç»ƒ

é¢„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ 5.91 äº¿ä»½æ–‡æ¡£å’Œ 1930 äº¿ä¸ªtokenï¼ŒåŒ…æ‹¬å…­ä¸ªå­æ•°æ®é›†ï¼š

* CFData-CPï¼ˆ6.24%ï¼‰ï¼šåŒ…æ‹¬ 3,900 ä»½å…¬å¸æ‹›è‚¡è¯´æ˜ä¹¦ï¼Œå…±è®¡ 130 äº¿ä¸ªtokenï¼›
* CFData-CAï¼ˆ12.28%ï¼‰ï¼šåŒ…æ‹¬ 600 ä¸‡ä»½å…¬å¸å…¬å‘Šï¼Œå…±è®¡ 170 äº¿ä¸ªtokenï¼›
* CFData-RRï¼ˆ2.51%ï¼‰ï¼šåŒ…æ‹¬ 39.2 ä¸‡ä»½ç ”ç©¶æŠ¥å‘Šï¼Œå…±è®¡ 30 äº¿ä¸ªtokenï¼›
* CFData-FNï¼ˆ18.70%ï¼‰ï¼šåŒ…æ‹¬ 8,200 ä¸‡ä»½è´¢ç»æ–°é—»ï¼Œå…±è®¡ 260 äº¿ä¸ªtokenï¼›
* CFData-SMï¼ˆ60.15%ï¼‰ï¼šåŒ…æ‹¬ 4.95 äº¿ä»½ç¤¾äº¤åª’ä½“å†…å®¹ï¼Œå…±è®¡ 840 äº¿ä¸ªtokenï¼›
* CFData-Wikiï¼ˆ0.09%ï¼‰ï¼šåŒ…æ‹¬ 25.5 ä¸‡ä»½ç»´åŸºç™¾ç§‘å†…å®¹ï¼Œå…±è®¡ 1.37 äº¿ä¸ªtokenã€‚

æˆ‘ä»¬ä»CFData-ptä¸­æŠ½å–äº†ä¸€ä¸ªè´¢ç»æ–‡æœ¬å­è¯­æ–™åº“ï¼Œä»¥ä¾¿åœ¨InternLM-7Bä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„é¢„è®­ç»ƒã€‚è¯¥å­è¯­æ–™åº“åŒ…å«äº†æ¥è‡ªå¤§é‡ä¸­å›½è´¢ç»æ•°æ®å’Œåˆ†æä»¥åŠå°‘é‡é€šç”¨æ–‡æœ¬çš„å…±è®¡çº¦137äº¿ä¸ªtokenï¼Œè¿™äº›é€šç”¨æ–‡æœ¬åŒ…æ‹¬å…¬å‘Šã€ç ”ç©¶æŠ¥å‘Šã€ç¤¾äº¤åª’ä½“å†…å®¹ã€è´¢ç»æ–°é—»æ–‡ç« å’Œç»´åŸºç™¾ç§‘ç­‰ï¼Œè€Œè¿™äº›æ•°æ®ä¸»è¦ç”±æˆ‘ä»¬è‡ªè¡Œæ”¶é›†ã€‚

## æœ‰ç›‘ç£å¾®è°ƒ

ç›‘ç£å¾®è°ƒæ•°æ®é›†åŒ…æ‹¬160ä¸‡æ¡æŒ‡ä»¤å¯¹å’Œ15äº¿ä¸ªæ ‡è®°ï¼Œå…¶ä¸­åŒ…æ‹¬å…­ä¸ªé‡‘èä»»åŠ¡ï¼š
* CFData-SAï¼ˆ5.69%ï¼‰ï¼š12ä¸‡ä¸ªå®ä¾‹ï¼Œ8600ä¸‡æ ‡è®°ç”¨äºæƒ…æ„Ÿåˆ†æï¼›
* CFData-RSï¼ˆ50.60%ï¼‰ï¼š36.9ä¸‡ä¸ªå®ä¾‹ï¼Œ7.65äº¿æ ‡è®°ç”¨äºæŠ¥å‘Šæ‘˜è¦ï¼›
* CFData-EDï¼ˆ22.69%ï¼‰ï¼š49ä¸‡ä¸ªå®ä¾‹ï¼Œ3.43äº¿æ ‡è®°ç”¨äºäº‹ä»¶æ£€æµ‹ï¼›
* CFData-TDï¼ˆ12.37%ï¼‰ï¼š36.9ä¸‡ä¸ªå®ä¾‹ï¼Œ1.87äº¿æ ‡è®°ç”¨äºä¸»é¢˜åˆ†è§£ï¼›
* CFData-QAï¼ˆ0.39%ï¼‰ï¼š1.2ä¸‡ä¸ªå®ä¾‹ï¼Œ600ä¸‡æ ‡è®°ç”¨äºé—®ç­”ï¼›
* CFData-SPï¼ˆ8.27%ï¼‰ï¼š21.2ä¸‡ä¸ªå®ä¾‹ï¼Œ1.25äº¿æ ‡è®°ç”¨äºè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€‚

æˆ‘ä»¬åˆ©ç”¨é«˜è´¨é‡çš„é¢†åŸŸç‰¹å®šæ•°æ®ï¼Œé€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒæ¥å®ç°é‡‘èé¢†åŸŸçš„é€‚åº”æ€§ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬å…­ä¸ªé‡‘èæ•°æ®é›†ï¼Œä»¥åæ˜ é‡‘èåˆ†æå’Œå†³ç­–çš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€äº‹ä»¶æ£€æµ‹ã€æŠ¥å‘Šæ‘˜è¦ã€ä¸»é¢˜åˆ†è§£ã€é—®é¢˜å›ç­”å’Œè‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚

CFData-sftæä¾›äº†å¤§é‡é‡‘èé¢†åŸŸçš„æ–‡æœ¬ä¿¡æ¯ï¼Œä½¿FinLLMèƒ½å¤Ÿä»ä¸åŒçš„ä¿¡æ¯æºä¸­å­¦ä¹ ã€‚

è€ƒè™‘åˆ°å®é™…éœ€æ±‚ï¼Œæˆ‘ä»¬å°†è¿™äº›é‡‘èæœ‰ç›‘ç£å¾®è°ƒæ•°æ®é›†é‡ç»„æˆåä¸ªä»»åŠ¡ã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†ä¿¡æ¯ï¼š
| ä»»åŠ¡ | ä»»åŠ¡æè¿° | æ•°æ®é›† | å¤§å° |
| - | - | - | - |
| Sentiment | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„æƒ…æ„Ÿ | CFData-SA | 13K |
| Summary | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”Ÿæˆå†…å®¹æ‘˜è¦ | CFData-RS | 18K |
| Risk | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”Ÿæˆé£é™©è­¦æŠ¥ | CFData-RS | 20K |
| Suggestion | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”ŸæˆæŠ•èµ„å»ºè®® | CFData-RS | 18K |
| Event | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„äº‹ä»¶ç±»åˆ« | CFData-ED | 12K |
| Industry | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„è¡Œä¸šç±»åˆ« | CFData-ED | 14K |
| Company | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„å…¬å¸åç§° | CFData-ED | 12K |
| Product | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„äº§å“åç§° | CFData-ED | 21K |
| Exam | å›ç­”ä¸è´¢åŠ¡é—®é¢˜ç›¸å…³çš„æ˜¯éé—®é¢˜ | CFData-QA | 16K |
| Stock | é¢„æµ‹è‚¡ç¥¨æœªæ¥èµ°åŠ¿ | CFData-SP | 15K |

å› ä¸ºæ•°æ®çš„è®¸å¯é—®é¢˜, æˆ‘ä»¬ä¸èƒ½å…¬å¼€å‘å¸ƒå®Œæ•´ç‰ˆçš„CFData. 
ç ”ç©¶äººå‘˜å¯ä»¥å‚è€ƒæˆ‘ä»¬[CFData](./data)çš„ä¸€äº›ç¤ºä¾‹æ•°æ®

# ä»£ç 

## ç»§ç»­é¢„è®­ç»ƒ

è®­ç»ƒè„šæœ¬åœ¨ **`./code/train/pretrain`**

```bash
deepspeed --include localhost:0,1,2,3,4,5,6,7 --master_port 60002 bf_16_parallel_train.py --config bf_16_parallel_train.yml > bf_16_parallel_train.log 2>&1
```

<div align="center">
<img align="center" src=./figs/CFGPT-Training-loss.svg width="100%"/>
</div>

trainerçš„è®­ç»ƒå‚æ•°åœ¨ **`./code/train/pretrain/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/further/pretrain/model
dataset: path/to/your/further/pretrain/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./bf_16_parallel_train
logging_steps: 10
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 1000
save_steps: 1000
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
remove_unused_columns: 0
```

Deepspeed è®­ç»ƒå‚æ•°åœ¨ **`./code/train/pretrain/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,
    "optimizer": {
        "type": "AdamW",
        "params": {
          "lr": "auto",
          "betas": "auto",
          "eps": "auto",
          "weight_decay": 0.01
          }
        },
     "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": "auto",
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 1,
        "reduce_bucket_size": 5e8
    }
}
```

## æœ‰ç›‘ç£å¾®è°ƒ

è®­ç»ƒè„šæœ¬ä½äº **`./code/train/lora`** ç›®å½•ä¸‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ lora-bf16 ä½œä¸ºç¤ºä¾‹ã€‚

```bash
deepspeed --include localhost:6,7 --master_port 60005 lora_bf_16_parallel_train.py --config lora_bf_16_parallel_train.yml > lora_bf_16_parallel_train.log 2>&1
```

Trainer è®­ç»ƒå‚æ•°åœ¨ **`./code/train/lora/bf16/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/supervised/finetuning/model
dataset: path/to/your/supervised/finetuning/dataset
dataset_eval: path/to/your/evaluate/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./lora_bf_16_parallel_train
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 500
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
save_steps: 500
evaluation_strategy: steps
eval_steps: 100
logging_steps: 10
remove_unused_columns: 0

# lora setting
rank: 64
lora_alpha: 16
lora_dropout: 0.05
target_modules: ['k_proj', 'o_proj', 'down_proj', 'v_proj', 'q_proj', 'gate_proj', 'up_proj']
bias: 'none'

# restart info
resume_from_checkpoint: null
```

Deepspeedè®­ç»ƒå‚æ•°åœ¨ **`./code/train/lora/bf16/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,

    "optimizer": {
      "type": "AdamW",
      "params": {
        "lr": "auto",
        "betas": "auto",
        "eps": "auto",
        "weight_decay": "auto"
        }
      
      },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
      "enabled": true
    },
    "zero_optimization": {
        "stage": 0
    }
}
```

# è¯„æµ‹

CFGPT2çš„è¯„æµ‹ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚

## C-Eval

| Model              | Size | STEM      | Social Science | Humanities | Others  | Average | Average(hard) |
| ------------------ | ---- | -------   | ------         | -----      | ------  | -----   | -------       |
| GPT-4              | -    | **67.1**  | 77.6           | 64.5       | 67.8    | 68.7    | **54.9**      |
| ChatGPT            | 175B | 52.9      | 61.8           | 50.9       | 53.6    | 54.4    | 41.4          |
| InternLM-7B        | 7B   | 48.0      | 67.4           | 55.4       | 45.8    | 52.8    | 37.1          |
| ChatGLM2-6B        | 6B   | 48.6      | 60.5           | 51.3       | 49.8    | 51.7    | 37.1          |
| Qwen-7B            | 7B   | 52.8      | 74.1           | 63.1       | 55.2    | 59.6    | 41.0          |
| Qwen-14B           | 14B  | 65.7      | **85.4**       | **75.3**   | **68.4**| **72.1**| 53.7          |
| Baichuan-7B        | 7B   | 38.2      | 52.0           | 46.2       | 39.3    | 42.8    | 31.5          |
| Baichuan-13B       | 13B  | 47.0      | 66.8           | 57.3       | 49.8    | 53.6    | 36.7          |
| Baichuan2-13B-Chat | 13B  | 48.4      | 70.5           | 60.3       | 55.0    | 56.6    | 37.9          |
| CFGPT-2            | 13B  | 47.6      | 70.0           | 60.7       | 54.5    | 56.2    | 33.7          |

## FinEval

| Model              | Size | Finance | Economy | Accounting | Certificate | Average | 
| ------------------ | ---- | ------- | ------  | -----      | ---------   | ---     |
| GPT-4              | -    | **71.0**| **74.5**| **59.3**   | **70.4**    | **68.6**| 
| ChatGPT            | 175B | 59.3    | 61.6    | 45.2       | 55.1        | 55.0    | 
| InternLM-7B        | 7B   | 49.0    | 49.2    | 40.5       | 49.4        | 47.1    | 
| ChatGLM2-6B        | 6B   | 46.5    | 46.4    | 44.5       | 51.5        | 47.4    | 
| Qwen-Chat-7B       | 7B   | 51.5    | 52.1    | 44.5       | 53.6        | 50.5    | 
| Qwen-7B            | 7B   | 54.5    | 54.4    | 50.3       | 55.8        | 53.8    | 
| Baichuan-7B-Chat   | 7B   | 44.9    | 41.5    | 34.9       | 45.6        | 42.0    | 
| Baichuan-13B-Chat  | 13B  | 51.6    | 51.1    | 41.7       | 52.8        | 49.4    | 
| CFGPT-2            | 13B  | 57.3    | 56.2    | 51.2       | 57.6        | 55.6    | 

## CFBenchmark-Basic
| Model              | Size | Company   | Product   | R.Avg     | Sector  | Event     | Sentiment | C.Avg     | Summary   | Risk      | Suggestion | G.Avg     | Avg       |
| ------------------ | ---- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | ---------- | --------- | --------- |
| HUMAN              | -    | 0.931     | 0.744     | 0.838     | 0.975     | 0.939     | 0.912     | 0.942     | 1.000     | 1.000     | 1.000      | 1.000     | 0.927     |
| ChatGPT            | 20B  | 0.797     | 0.198     | 0.498     | 0.453     | 0.458     | 0.425     | 0.455     | 0.593     | 0.541     | 0.771      | 0.635     | 0.529     |
| ERNIE-Bot          | 260B | 0.807     | 0.300     | 0.533     | 0.408     | 0.350     | 0.186     | 0.315     | 0.715     | 0.590     | 0.716      | 0.673     | 0.507     |
| ERNIE-Bot-4        | -    | 0.819     | 0.417     | 0.618     | 0.418     | 0.358     | 0.375     | 0.384     | 0.721     | 0.629     | 0.718      | 0.689     | 0.564     |
| Falcon-7B          | 7B   | 0.671     | 0.168     | 0.420     | 0.169     | 0.132     | 0.250     | 0.184     | 0.302     | 0.301     | 0.246      | 0.283     | 0.296     |
| Falcon-7B-chat     | 7B   | 0.582     | 0.046     | 0.314     | 0.112     | 0.142     | 0.153     | 0.135     | 0.307     | 0.299     | 0.258      | 0.288     | 0.246     |
| bloomz-7B1         | 7B   | 0.765     | 0.166     | 0.465     | 0.252     | 0.154     | 0.394     | 0.267     | 0.451     | 0.371     | 0.462      | 0.428     | 0.387     |
| bloomz-7Bt1-mt     | 7B   | 0.751     | 0.157     | 0.454     | 0.087     | 0.182     | 0.380     | 0.216     | 0.425     | 0.379     | 0.396      | 0.400     | 0.357     |
| Qwen-7B            | 7B   | 0.780     | 0.357     | 0.569     | 0.480     | 0.335     | 0.379     | 0.398     | 0.750     | 0.505     | 0.713      | 0.656     | 0.541     |
| Qwen-Chat-7B       | 7B   | 0.763     | 0.360     | 0.562     | 0.400     | 0.367     | 0.265     | 0.344     | 0.548     | 0.307     | 0.379      | 0.411     | 0.439     |
| Qwen-14B           | 14B  | 0.805     | 0.421     | 0.613     | 0.481     | 0.350     | 0.385     | 0.405     | 0.754     | 0.608     | 0.717      | 0.693     | 0.570     |
| Qwen-Chat-14B      | 14B  | 0.814     | 0.442     | 0.628     | 0.382     | 0.400     | 0.350     | 0.377     | 0.732     | 0.478     | 0.736      | 0.649     | 0.551     |
| ChatGLM2-6B        | 6B   | 0.747     | 0.313     | 0.530     | 0.285     | 0.300     | 0.357     | 0.314     | 0.657     | 0.454     | 0.671      | 0.594     | 0.479     |
| Baichuan2-7B-Base  | 7B   | 0.672     | 0.340     | 0.506     | 0.342     | 0.490     | 0.480     | 0.437     | 0.739     | 0.619     | 0.751      | 0.703     | 0.549     |
| Baichuan2-7B-Chat  | 7B   | 0.757     | 0.402     | 0.579     | 0.425     | 0.475     | 0.323     | 0.408     | 0.725     | 0.648     | 0.732      | 0.702     | 0.563     |
| Baichuan2-13B-Base | 13B  | 0.781     | 0.330     | 0.555     | 0.436     | 0.496     | 0.477     | 0.470     | 0.725     | 0.503     | 0.747      | 0.658     | 0.561     |
| Baichuan2-13B-Chat | 13B  | 0.797     | 0.314     | 0.556     | 0.472     | 0.507     | 0.387     | 0.455     | 0.739     | 0.634     | 0.746      | 0.706     | 0.572     |
| InternLM-7B        | 7B   | 0.612     | 0.233     | 0.423     | 0.266     | 0.311     | 0.328     | 0.302     | 0.378     | 0.336     | 0.379      | 0.364     | 0.363     |
| InternLM-7B-Chat   | 7B   | 0.632     | 0.261     | 0.447     | 0.272     | 0.364     | 0.399     | 0.345     | 0.363     | 0.270     | 0.353      | 0.329     | 0.374     |
| InternLM-20B       | 20B  | 0.809     | 0.358     | 0.583     | 0.500     | 0.427     | 0.417     | 0.448     | 0.706     | 0.653     | 0.728      | 0.695     | 0.575     |
| InternLM-20B-Chat  | 20B  | 0.488     | 0.362     | 0.425     | 0.323     | 0.327     | 0.370     | 0.340     | 0.706     | 0.578     | 0.762      | 0.662     | 0.476     |
| CFGPT1-stf-LoRA    | 7B   | 0.820     | 0.414     | 0.617     | 0.569     | 0.729     | 0.769     | 0.689     | 0.745     | 0.584     | 0.609      | 0.646     | 0.650     |
| CFGPT1-sft-Full    | 7B   | **0.836** | **0.476** | **0.656** | **0.700** | **0.808** | **0.829** | **0.779** | **0.798** | **0.669** | **0.808**  | **0.758** | **0.731** |
| CFGPT2             | 13B  |**0.861**|**0.490**|**0.676**|**0.722** |**0.835**|**0.831**  |**0.796**|**0.821**|**0.723**|**0.831**   |**0.792**|**0.755**|

## OpenFinData

| Model              | Size | Knowledge | Caluation | Explanation | Identification | Analysis | Compliance | Average | 
| ------------------ | ---- | -------   | ------    | -----       | ---------      | -----    | -------    | -----   |
| ERNIE-Bot-3.5      | -    | 78.0      | 70.4      | 82.1        | 75.3           | 77.7     | 36.7       | 70.0    | 
| ERNIE-Bot-4        | -    | **87.3**  | **73.6**  | **84.3**    | **77.0**       | **79.1** | 37.3       |**73.1** | 
| InternLM-7B        | 7B   | 65.3      | 45.8      | 71.4        | 62.5           | 59.2     | 37.2       | 56.9    | 
| ChatGLM2-6B        | 6B   | 62.4      | 37.2      | 70.8        | 59.2           | 58.3     | 38.7       | 54.4    | 
| Qwen-Chat-7B       | 7B   | 71.3      | 40.5      | 71.4        | 58.6           | 51.3     | 40.0       | 55.5    | 
| Qwen-Chat-14B      | 14B  | 78.0      | 57.6      | 75.6        | 71.6           | 59.3     | 40.6       | 63.8    | 
| Baichuan2-7B-Chat  | 7B   | 46.2      | 37.0      | 76.5        | 60.2           | 55.0     | 28.7       | 50.6    | 
| Baichuan2-13B-Chat | 13B  | 69.3      | 39.5      | 75.3        | 65.7           | 62.0     | 31.3       | 57.2    | 
| CFGPT-2            | 13B  | 86.7      | 64.3      | 77.3        | 73.8           | 65.2     |**70.2**    | 72.9    | 


# è‡´è°¢

CFGPTå·²å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬è¦å‘è¿™äº›é¡¹ç›®çš„ç ”ç©¶è€…è¡¨ç¤ºæ„Ÿè°¢å’Œå°Šé‡ã€‚

- InternLM: https://github.com/InternLM/InternLM
- Firefly: https://github.com/yangjianxin1/Firefly
- FinGPT: https://github.com/AI4Finance-Foundation/FinGPT


# æœªæ¥å·¥ä½œ
- [ ] ä½¿ç”¨CFGPTåˆ›å»ºä¸‹æ¸¸çš„åº”ç”¨CFAPP
- [ ] æ„å»ºæ›´åŠ å…¨é¢çš„è®­ç»ƒä»»åŠ¡ä¸å¯¹åº”æ•°æ®
- [ ] æŒç»­æ€§æ”¹è¿›CFGPTåœ¨æ›´å¤šå¤æ‚é‡‘èä»»åŠ¡ä¸Šçš„èƒ½åŠ›

# ä½¿ç”¨è®¸å¯
CFGPTçš„ä»£ç éµå¾ªApacheè®¸å¯è¯2.0åè®®ã€‚CFGPTçš„æ¨¡å‹å…è´¹å¼€æºï¼Œå•†ç”¨è®¸å¯éµå¾ªå¼€æºåŸºç¡€æ¨¡å‹InternLM 7Bå’Œç™¾å·2-13Bçš„è®¸å¯åè®®å’ŒOpenAIç”Ÿæˆæ•°æ®çš„ä½¿ç”¨æ¡æ¬¾ã€‚å¦‚æœæ‚¨å‘ç°ä»»ä½•æ½œåœ¨çš„è¿è§„è¡Œä¸ºï¼Œè¯·ä¸æˆ‘ä»¬è”ç³»ã€‚

### æ„Ÿè°¢æˆ‘ä»¬çš„è´¡çŒ®è€… :
<a href="https://github.com/TongjiFinLab/CFGPT/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=TongjiFinLab/CFGPT" />
</a>

# å¼•ç”¨
å¦‚æœæ‚¨è®¤ä¸º**CFGPT**å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œå¯ä»¥å¼•ç”¨ä»¥ä¸‹çš„è®ºæ–‡ï¼š

```
@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
```
@article{li2024racfgpt,
         author = {Jiangtong Li, Yang Lei, Yuxuan Bian, Dawei Cheng, Zhijun Ding, Changjun Jiang},
         title = {RA-CFGPT: Chinese Financial Assistant with Retrieval-Augmented Large Language Model},
         publisher = {Front. Comput. Sci.},
         year = {2024},
         journal = {Frontiers of Computer Science},
         volume = {},
         number = {},
         doi = {10.1007/s11704-024-31018-5}
}    
```