<div style="text-align:center">
<!-- <img src="https://big-cheng.com/k2/k2.png" alt="k2-logo" width="200"/> -->
<h2>ğŸ“ˆ CFGPT: Chinese Financial Assistant with Large Language Model</h2>
</div>

<a href='https://arxiv.org/abs/2309.10654'><img src='https://img.shields.io/badge/Paper-ArXiv-C71585'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(pt)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20LoRA)-red'></a> 
<a href='https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging Face-CFGPT(sft%20Full)-red'></a> 

[English](README.md) | ç®€ä½“ä¸­æ–‡

# ç®€ä»‹

**CFGPT**æ˜¯ä¸€ä¸ªå¼€æºçš„è¯­è¨€æ¨¡å‹ï¼Œé¦–å…ˆé€šè¿‡åœ¨æ”¶é›†å’Œæ¸…ç†çš„ä¸­å›½é‡‘èæ–‡æœ¬æ•°æ®ï¼ˆCFData-ptï¼‰ä¸Šè¿›è¡Œç»§ç»­é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬é‡‘èé¢†åŸŸç‰¹å®šæ•°æ®ï¼ˆå…¬å‘Šã€é‡‘èæ–‡ç« ã€é‡‘èè€ƒè¯•ã€é‡‘èæ–°é—»ã€é‡‘èç ”ç©¶è®ºæ–‡ï¼‰å’Œé€šç”¨æ•°æ®ï¼ˆç»´åŸºç™¾ç§‘ï¼‰ï¼Œç„¶åä½¿ç”¨çŸ¥è¯†å¯†é›†çš„æŒ‡å¯¼è°ƒæ•´æ•°æ®ï¼ˆCFData-sftï¼‰è¿›è¡Œå¾®è°ƒã€‚
æˆ‘ä»¬ä½¿ç”¨CFBenchmark-Basicè¿›è¡Œåˆæ­¥è¯„ä¼°ã€‚ä¸å‡ ä¸ªå…·æœ‰ç›¸ä¼¼å‚æ•°çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCFGPTåœ¨è¯†åˆ«ï¼Œåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚

åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†äº«ä»¥ä¸‹çš„æ¨¡å‹å’Œä»£ç ã€‚

- æˆ‘ä»¬å°†CFGPT1 (7B) åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†å‘å¸ƒï¼š
    - [Pretrained Model](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B): åœ¨ä¸­å›½é‡‘èæ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œè¿›ä¸€æ­¥é¢„è®­ç»ƒä¸”ç¬¦åˆInternLMæ¨¡å‹è®¸å¯çš„å®Œæ•´æ¨¡å‹æƒé‡ã€‚
    - [Supervised Finetuned Model (Lora)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA): åŸºäºæˆ‘ä»¬ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹çš„ç”±PEFTï¼ˆLoRAï¼‰è®­ç»ƒçš„é€‚é…å™¨æ¨¡å‹æƒé‡ã€‚
    - [Supervised Finetuned Model (Full)](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full): åŸºäºæˆ‘ä»¬ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹çš„è¿›ä¸€æ­¥å…¨å‚æ•°å¾®è°ƒçš„å®Œæ•´æ¨¡å‹è®­ç»ƒæƒé‡ã€‚

- æˆ‘ä»¬è¿˜å‘å¸ƒäº†[CFBenchmark](https://github.com/TongjiFinLab/CFBenchmark)ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡é‡‘èåŸºå‡†æµ‹è¯•ã€‚åŸºç¡€ç‰ˆæœ¬çš„CFBenchmarkåŒ…æ‹¬3917ä¸ªé‡‘èæ–‡æœ¬ï¼Œæ¶µç›–ä¸‰ä¸ªæ–¹é¢å’Œå…«ä¸ªä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°ä¸­æ–‡é‡‘èå¸‚åœºä¸­LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é‡‘èæ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚

- æˆ‘ä»¬è¿˜å‘å¸ƒäº†CFGPTçš„è¿›ä¸€æ­¥é¢„è®­ç»ƒå’ŒæŒ‡å¯¼å¾®è°ƒçš„ä»£ç ã€‚

- æˆ‘ä»¬è¿˜æä¾›äº†CFData-sftçš„ç›¸å…³ç¤ºä¾‹æ•°æ®ï¼Œä»¥æ–¹ä¾¿ä½¿ç”¨è€…ç†è§£æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹

***ä»¥ä¸‹æ˜¯è®­ç»ƒ CFGPT çš„æµç¨‹æ¦‚è§ˆå›¾ï¼š***

<div align="center">
<img align="center" src=./figs/CFGPT-Training.svg width="100%"/>
</div>

# ç›®å½•

- [å¿«é€Ÿä½¿ç”¨](#å¿«é€Ÿä½¿ç”¨)
- [å…¸å‹ä½¿ç”¨æ¡ˆä¾‹](#å…¸å‹ä½¿ç”¨æ¡ˆä¾‹)
- [æ•°æ®é›†](#æ•°æ®é›†)
- [ä»£ç ](#ä»£ç )
- [è¯„æµ‹åŸºå‡†](#è¯„æµ‹åŸºå‡†)
- [è‡´è°¢](#è‡´è°¢)
- [æœªæ¥å·¥ä½œ](#æœªæ¥å·¥ä½œ)
- [ä½¿ç”¨è®¸å¯](#ä½¿ç”¨è®¸å¯)
- [å¼•ç”¨](#å¼•ç”¨)

# å¿«é€Ÿä½¿ç”¨

**1. å‡†å¤‡ä»£ç å’Œç¯å¢ƒ**

å…‹éš†æˆ‘ä»¬çš„ä»“åº“ï¼Œåˆ›å»ºä¸€ä¸ªPythonç¯å¢ƒï¼Œå¹¶é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¿€æ´»å®ƒï¼š
```bash
git clone https://github.com/TongjiFinLab/CFGPT.git
cd CFGPT
conda create -n env_name python=3.10   
source activate env_name 
pip install -r requirements.txt
```

**2. å‡†å¤‡é¢„è®­ç»ƒçš„ CFGPT1**

CFGPT1ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†InternLM-7Båœ¨æˆ‘ä»¬çš„CFData-ptä¸Šç»§ç»­é¢„è®­ç»ƒï¼Œä¸€ä¸ªLoRAæ¨¡å‹ï¼ˆé€šè¿‡PEFTåœ¨æˆ‘ä»¬çš„CFData-sftä¸Šè®­ç»ƒï¼‰ï¼Œä»¥åŠåŸºäºç»§ç»­é¢„è®­ç»ƒæ¨¡å‹ç›‘ç£å¾®è°ƒè®­ç»ƒçš„å…¨ç²¾è°ƒæ¨¡å‹ã€‚

|Pretrain model|Adapter model|Full SFT Model|
|:-:|:-:|:-:|
 [CFGPT1-pt-7B](https://huggingface.co/TongjiFinLab/CFGPT1-pt-7B)|[CFGPT1-sft-7B-LoRA](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-LoRA)|[CFGPT1-sft-7B-Full](https://huggingface.co/TongjiFinLab/CFGPT1-sft-7B-Full)|

**3. ä½¿ç”¨ CFGPT1-sft-7B-LoRA**

```python
from transformers import AutoModel, AutoTokenizer
from peft import PeftModel
base_model = 'TongjiFinLab/CFGPT1-pt-7B'
lora_weights = 'TongjiFinLab/CFGPT1-sft-7B-LoRA'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = PeftModel.from_pretrained(
    model,
    lora_weights,
    device_map=device_map,
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

**4. ä½¿ç”¨ CFGPT1-sft-7B-Full**

```python
from transformers import AutoModel, AutoTokenizer
base_model = 'TongjiFinLab/CFGPT1-sft-7B-Full'
device_map = 'cuda:0'
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
model = AutoModel.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map=device_map,
    torch_dtype=torch.bfloat16
)
model = model.eval()
inputs = tokenizer("""ä½ æ˜¯ä¸€åé‡‘èä»ä¸šè€…ï¼Œè¯·å¯¹è¿™ç¯‡æ–°é—»è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚è¯·ä»ï¼ˆä¸­æ€§ã€ç§¯æã€æ¶ˆæï¼‰ä¸­é€‰å–ç­”æ¡ˆã€‚æ–°é—»å†…å®¹ï¼šæŒ–è´å¿«è®¯ï¼šç‰¹æ­¥å›½é™…å‘å¸ƒ2023å¹´ç¬¬äºŒå­£åº¦ä¸­å›½å†…åœ°ä¸šåŠ¡è¥è¿çŠ¶å†µï¼ŒæŠ«éœ²æˆªè‡³2023å¹´6æœˆ30æ—¥æ­¢3ä¸ªæœˆé›¶å”®é”€å”®å®ç°é«˜åŒä½æ•°åŒæ¯”å¢é•¿(åŒ…æ‹¬çº¿ä¸Šçº¿ä¸‹æ¸ é“)ï¼Œé›¶å”®æŠ˜æ‰£æ°´å¹³çº¦ä¸ƒäº”æŠ˜ã€‚åŒæ—¶ï¼Œ2022å¹´7æœˆMSCIé¦–æ¬¡äºˆä»¥ç‰¹æ­¥ESGè¯„çº§ï¼Œä¸€å¹´åè¯„çº§è¡¨ç°å³è¿æ¥æå‡ã€‚æ˜æ™ŸMSCIä¸Šè°ƒç‰¹æ­¥ESGè¯„çº§ï¼Œç”±â€œBBâ€å‡è‡³â€œBBBâ€ã€‚\nå›ç­”ï¼š""", return_tensors='pt').to(device_map)
pred = model.generate(**inputs, max_new_tokens=64, do_sample=False, repetition_penalty=1.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('å›ç­”ï¼š')[1])
```

- **æ›´å¤šä½¿ç”¨ç»†èŠ‚åœ¨ `./code/test`**

# å…¸å‹ä½¿ç”¨æ¡ˆä¾‹

- [CFGPT-v2-13B é“¶è¡Œä¸šåœºæ™¯ä½¿ç”¨æ¡ˆä¾‹](cases/case_bank.md)
- [CFGPT-v1-7B å…¸å‹æ•°æ®æ¡ˆä¾‹ä»‹ç»](cases/case_CFGPTv1.md)

# æ•°æ®é›†

åœ¨è¿™ä¸ªå­˜å‚¨åº“ä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†CFDataçš„æ ·æœ¬ï¼š
- CFDataï¼š`./data`

    CFDataåŒ…æ‹¬ä¸€ä¸ªé¢„è®­ç»ƒæ•°æ®é›†ï¼ˆCFData-ptï¼‰å’Œä¸€ä¸ªç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼ˆCFData-sftï¼‰ï¼Œå…¶ä¸­é¢„è®­ç»ƒæ•°æ®é›†æ±‡é›†äº†ä¸­å›½é‡‘èæ•°æ®å’Œåˆ†æï¼Œä»¥åŠä¸€ä¸ªè¾ƒå°çš„é€šç”¨æ–‡æœ¬å­é›†ï¼Œæ€»å…±æœ‰5.84äº¿ä¸ªæ–‡æ¡£å’Œ1410äº¿ä¸ªtokenï¼Œç›‘ç£å¾®è°ƒæ•°æ®é›†ä¸“ä¸ºå…­ç§ä¸åŒçš„é‡‘èä»»åŠ¡é‡èº«å®šåˆ¶ï¼Œæ¶µç›–äº†é‡‘èåˆ†æå’Œå†³ç­–åˆ¶å®šçš„å„ä¸ªæ–¹é¢ï¼Œå…±æœ‰150ä¸‡ä¸ªæŒ‡ä»¤å¯¹å’Œ150äº¿ä¸ªtokenã€‚


## ç»§ç»­é¢„è®­ç»ƒ

é¢„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ 5.91 äº¿ä»½æ–‡æ¡£å’Œ 1930 äº¿ä¸ªtokenï¼ŒåŒ…æ‹¬å…­ä¸ªå­æ•°æ®é›†ï¼š

* CFData-CPï¼ˆ6.24%ï¼‰ï¼šåŒ…æ‹¬ 3,900 ä»½å…¬å¸æ‹›è‚¡è¯´æ˜ä¹¦ï¼Œå…±è®¡ 130 äº¿ä¸ªtokenï¼›
* CFData-CAï¼ˆ12.28%ï¼‰ï¼šåŒ…æ‹¬ 600 ä¸‡ä»½å…¬å¸å…¬å‘Šï¼Œå…±è®¡ 170 äº¿ä¸ªtokenï¼›
* CFData-RRï¼ˆ2.51%ï¼‰ï¼šåŒ…æ‹¬ 39.2 ä¸‡ä»½ç ”ç©¶æŠ¥å‘Šï¼Œå…±è®¡ 30 äº¿ä¸ªtokenï¼›
* CFData-FNï¼ˆ18.70%ï¼‰ï¼šåŒ…æ‹¬ 8,200 ä¸‡ä»½è´¢ç»æ–°é—»ï¼Œå…±è®¡ 260 äº¿ä¸ªtokenï¼›
* CFData-SMï¼ˆ60.15%ï¼‰ï¼šåŒ…æ‹¬ 4.95 äº¿ä»½ç¤¾äº¤åª’ä½“å†…å®¹ï¼Œå…±è®¡ 840 äº¿ä¸ªtokenï¼›
* CFData-Wikiï¼ˆ0.09%ï¼‰ï¼šåŒ…æ‹¬ 25.5 ä¸‡ä»½ç»´åŸºç™¾ç§‘å†…å®¹ï¼Œå…±è®¡ 1.37 äº¿ä¸ªtokenã€‚

æˆ‘ä»¬ä»CFData-ptä¸­æŠ½å–äº†ä¸€ä¸ªè´¢ç»æ–‡æœ¬å­è¯­æ–™åº“ï¼Œä»¥ä¾¿åœ¨InternLM-7Bä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„é¢„è®­ç»ƒã€‚è¯¥å­è¯­æ–™åº“åŒ…å«äº†æ¥è‡ªå¤§é‡ä¸­å›½è´¢ç»æ•°æ®å’Œåˆ†æä»¥åŠå°‘é‡é€šç”¨æ–‡æœ¬çš„å…±è®¡çº¦137äº¿ä¸ªtokenï¼Œè¿™äº›é€šç”¨æ–‡æœ¬åŒ…æ‹¬å…¬å‘Šã€ç ”ç©¶æŠ¥å‘Šã€ç¤¾äº¤åª’ä½“å†…å®¹ã€è´¢ç»æ–°é—»æ–‡ç« å’Œç»´åŸºç™¾ç§‘ç­‰ï¼Œè€Œè¿™äº›æ•°æ®ä¸»è¦ç”±æˆ‘ä»¬è‡ªè¡Œæ”¶é›†ã€‚

## æœ‰ç›‘ç£å¾®è°ƒ

ç›‘ç£å¾®è°ƒæ•°æ®é›†åŒ…æ‹¬160ä¸‡æ¡æŒ‡ä»¤å¯¹å’Œ15äº¿ä¸ªæ ‡è®°ï¼Œå…¶ä¸­åŒ…æ‹¬å…­ä¸ªé‡‘èä»»åŠ¡ï¼š
* CFData-SAï¼ˆ5.69%ï¼‰ï¼š12ä¸‡ä¸ªå®ä¾‹ï¼Œ8600ä¸‡æ ‡è®°ç”¨äºæƒ…æ„Ÿåˆ†æï¼›
* CFData-RSï¼ˆ50.60%ï¼‰ï¼š36.9ä¸‡ä¸ªå®ä¾‹ï¼Œ7.65äº¿æ ‡è®°ç”¨äºæŠ¥å‘Šæ‘˜è¦ï¼›
* CFData-EDï¼ˆ22.69%ï¼‰ï¼š49ä¸‡ä¸ªå®ä¾‹ï¼Œ3.43äº¿æ ‡è®°ç”¨äºäº‹ä»¶æ£€æµ‹ï¼›
* CFData-TDï¼ˆ12.37%ï¼‰ï¼š36.9ä¸‡ä¸ªå®ä¾‹ï¼Œ1.87äº¿æ ‡è®°ç”¨äºä¸»é¢˜åˆ†è§£ï¼›
* CFData-QAï¼ˆ0.39%ï¼‰ï¼š1.2ä¸‡ä¸ªå®ä¾‹ï¼Œ600ä¸‡æ ‡è®°ç”¨äºé—®ç­”ï¼›
* CFData-SPï¼ˆ8.27%ï¼‰ï¼š21.2ä¸‡ä¸ªå®ä¾‹ï¼Œ1.25äº¿æ ‡è®°ç”¨äºè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ã€‚

æˆ‘ä»¬åˆ©ç”¨é«˜è´¨é‡çš„é¢†åŸŸç‰¹å®šæ•°æ®ï¼Œé€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒæ¥å®ç°é‡‘èé¢†åŸŸçš„é€‚åº”æ€§ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬å…­ä¸ªé‡‘èæ•°æ®é›†ï¼Œä»¥åæ˜ é‡‘èåˆ†æå’Œå†³ç­–çš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€äº‹ä»¶æ£€æµ‹ã€æŠ¥å‘Šæ‘˜è¦ã€ä¸»é¢˜åˆ†è§£ã€é—®é¢˜å›ç­”å’Œè‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚

CFData-sftæä¾›äº†å¤§é‡é‡‘èé¢†åŸŸçš„æ–‡æœ¬ä¿¡æ¯ï¼Œä½¿FinLLMèƒ½å¤Ÿä»ä¸åŒçš„ä¿¡æ¯æºä¸­å­¦ä¹ ã€‚

è€ƒè™‘åˆ°å®é™…éœ€æ±‚ï¼Œæˆ‘ä»¬å°†è¿™äº›é‡‘èæœ‰ç›‘ç£å¾®è°ƒæ•°æ®é›†é‡ç»„æˆåä¸ªä»»åŠ¡ã€‚

ä»¥ä¸‹æ˜¯è¯¦ç»†ä¿¡æ¯ï¼š
| ä»»åŠ¡ | ä»»åŠ¡æè¿° | æ•°æ®é›† | å¤§å° |
| - | - | - | - |
| Sentiment | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„æƒ…æ„Ÿ | CFData-SA | 13K |
| Summary | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”Ÿæˆå†…å®¹æ‘˜è¦ | CFData-RS | 18K |
| Risk | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”Ÿæˆé£é™©è­¦æŠ¥ | CFData-RS | 20K |
| Suggestion | åŸºäºæä¾›çš„è´¢åŠ¡æ–‡ä»¶ç”ŸæˆæŠ•èµ„å»ºè®® | CFData-RS | 18K |
| Event | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„äº‹ä»¶ç±»åˆ« | CFData-ED | 12K |
| Industry | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„è¡Œä¸šç±»åˆ« | CFData-ED | 14K |
| Company | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„å…¬å¸åç§° | CFData-ED | 12K |
| Product | è¯†åˆ«ä¸è´¢åŠ¡æ–‡ä»¶ç›¸å…³çš„äº§å“åç§° | CFData-ED | 21K |
| Exam | å›ç­”ä¸è´¢åŠ¡é—®é¢˜ç›¸å…³çš„æ˜¯éé—®é¢˜ | CFData-QA | 16K |
| Stock | é¢„æµ‹è‚¡ç¥¨æœªæ¥èµ°åŠ¿ | CFData-SP | 15K |

å› ä¸ºæ•°æ®çš„è®¸å¯é—®é¢˜, æˆ‘ä»¬ä¸èƒ½å…¬å¼€å‘å¸ƒå®Œæ•´ç‰ˆçš„CFData. 
ç ”ç©¶äººå‘˜å¯ä»¥å‚è€ƒæˆ‘ä»¬[CFData](./data)çš„ä¸€äº›ç¤ºä¾‹æ•°æ®

# ä»£ç 

## ç»§ç»­é¢„è®­ç»ƒ

è®­ç»ƒè„šæœ¬åœ¨ **`./code/train/pretrain`**

```bash
deepspeed --include localhost:0,1,2,3,4,5,6,7 --master_port 60002 bf_16_parallel_train.py --config bf_16_parallel_train.yml > bf_16_parallel_train.log 2>&1
```

<div align="center">
<img align="center" src=./figs/CFGPT-Training-loss.svg width="100%"/>
</div>

trainerçš„è®­ç»ƒå‚æ•°åœ¨ **`./code/train/pretrain/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/further/pretrain/model
dataset: path/to/your/further/pretrain/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./bf_16_parallel_train
logging_steps: 10
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 1000
save_steps: 1000
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
remove_unused_columns: 0
```

Deepspeed è®­ç»ƒå‚æ•°åœ¨ **`./code/train/pretrain/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,
    "optimizer": {
        "type": "AdamW",
        "params": {
          "lr": "auto",
          "betas": "auto",
          "eps": "auto",
          "weight_decay": 0.01
          }
        },
     "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "total_num_steps": "auto",
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 1,
        "reduce_bucket_size": 5e8
    }
}
```

## æœ‰ç›‘ç£å¾®è°ƒ

è®­ç»ƒè„šæœ¬ä½äº **`./code/train/lora`** ç›®å½•ä¸‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ lora-bf16 ä½œä¸ºç¤ºä¾‹ã€‚

```bash
deepspeed --include localhost:6,7 --master_port 60005 lora_bf_16_parallel_train.py --config lora_bf_16_parallel_train.yml > lora_bf_16_parallel_train.log 2>&1
```

Trainer è®­ç»ƒå‚æ•°åœ¨ **`./code/train/lora/bf16/bf_16_parallel_train.yml`**: 
```
# basic setting
model_name: path/of/your/supervised/finetuning/model
dataset: path/to/your/supervised/finetuning/dataset
dataset_eval: path/to/your/evaluate/dataset
deepspeed: ./ds_config.json
seed: 42
max_seq_length: 2048

# train setting 
output_dir: ./lora_bf_16_parallel_train
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 500
fp16: 0
bf16: 1
torch_compile: 0
save_strategy: steps
save_steps: 500
evaluation_strategy: steps
eval_steps: 100
logging_steps: 10
remove_unused_columns: 0

# lora setting
rank: 64
lora_alpha: 16
lora_dropout: 0.05
target_modules: ['k_proj', 'o_proj', 'down_proj', 'v_proj', 'q_proj', 'gate_proj', 'up_proj']
bias: 'none'

# restart info
resume_from_checkpoint: null
```

Deepspeedè®­ç»ƒå‚æ•°åœ¨ **`./code/train/lora/bf16/ds_config.json`**: 
```
{
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false,

    "optimizer": {
      "type": "AdamW",
      "params": {
        "lr": "auto",
        "betas": "auto",
        "eps": "auto",
        "weight_decay": "auto"
        }
      
      },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "bf16": {
      "enabled": true
    },
    "zero_optimization": {
        "stage": 0
    }
}
```

# è¯„æµ‹åŸºå‡†

æˆ‘ä»¬å‘å¸ƒäº†[CFBenchmark-Basic](https://github.com/TongjiFinLab/CFBenchmark)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­å›½é‡‘èåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬3917ä¸ªè·¨è¶³ä¸‰ä¸ªæ–¹é¢å’Œå…«ä¸ªä»»åŠ¡çš„é‡‘èæ–‡æœ¬ï¼Œç”¨äºè¯„ä¼°ä¸­æ–‡é‡‘èå¸‚åœºä¸ŠLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é‡‘èæ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚

CFBenchmark-Basicåˆ©ç”¨ä¸¤ç§ç±»å‹çš„æŒ‡æ ‡æ¥è¯„ä¼°LLMåœ¨é‡‘èé¢†åŸŸä¸Šçš„æ€§èƒ½ã€‚å¯¹äºè¯†åˆ«å’Œåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬é‡‡ç”¨**F1åˆ†æ•°**ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å¹³è¡¡ç²¾åº¦å’Œå¬å›ç‡ã€‚å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆç­”æ¡ˆçš„å‘é‡è¡¨ç¤ºä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„**ä½™å¼¦ç›¸ä¼¼åº¦**æ¥è¡¡é‡ç”Ÿæˆèƒ½åŠ›ã€‚ç”±äºåœ¨ç”Ÿæˆä»»åŠ¡ä¸­é€šå¸¸å­˜åœ¨ç€ç›¸ä¼¼å«ä¹‰çš„ä¸åŒè¡¨è¾¾ï¼Œç®€å•åœ°ä½¿ç”¨Rough-Score æˆ– BULE-socreæ˜¯ä¸åˆç†çš„ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬æŒ‡å®š**bge-zh-v1.5**ä½œä¸ºç”Ÿæˆå¥å­åµŒå…¥çš„æƒå¨æ¨¡å‹ã€‚æˆ‘ä»¬åˆ†åˆ«è®¡ç®—æ¯ä¸ªå­ä»»åŠ¡çš„è¯„ä¼°åˆ†æ•°ï¼Œå¹¶æä¾›æ¯ä¸ªç±»åˆ«çš„å¹³å‡åˆ†æ•°ã€‚

å„ç§LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„æœ€ä½³åˆ†æ•°ï¼ˆè€ƒè™‘zero-shotå’Œfew-shotï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š

| Model              | Size | Company | Product | R.Avg   | Industry | Event   | Sentiment | C.Avg   | Summary | Risk    | Suggestion | G.Avg   | Avg     |
| ------------------ | ---- | ------- | ------- | -----   | -------- | -----   | --------- | -----   | ------- | -----   | ---------- | -----   | -----   |
| ChatGPT            | 20B  | 0.797   | 0.198   | 0.498   | 0.453    | 0.458   | 0.425     | 0.455   | 0.593   | 0.541   | 0.771      | 0.635   | 0.529   |
| ERNIE-Bot          | 260B | 0.807   | 0.300   | 0.533   | 0.408    | 0.350   | 0.186     | 0.315   | 0.715   | 0.590   | 0.716      | 0.673   | 0.507   |
| ERNIE-Bot-4        | -    | 0.819   | 0.417   | 0.618   | 0.418    | 0.358   | 0.375     | 0.384   | 0.721   | 0.629   | 0.718      | 0.689   | 0.564   |
| Falcon-7B          | 7B   | 0.671   | 0.168   | 0.420   | 0.169    | 0.132   | 0.250     | 0.184   | 0.302   | 0.301   | 0.246      | 0.283   | 0.296   |
| Falcon-7B-chat     | 7B   | 0.582   | 0.046   | 0.314   | 0.112    | 0.142   | 0.153     | 0.135   | 0.307   | 0.299   | 0.258      | 0.288   | 0.246   |
| bloomz-7B1         | 7B   | 0.765   | 0.166   | 0.465   | 0.252    | 0.154   | 0.394     | 0.267   | 0.451   | 0.371   | 0.462      | 0.428   | 0.387   |
| bloomz-7Bt1-mt     | 7B   | 0.751   | 0.157   | 0.454   | 0.087    | 0.182   | 0.380     | 0.216   | 0.425   | 0.379   | 0.396      | 0.400   | 0.357   |
| Qwen-7B            | 7B   | 0.780   | 0.357   | 0.569   | 0.480    | 0.335   | 0.379     | 0.398   | 0.750   | 0.505   | 0.713      | 0.656   | 0.541   |
| Qwen-Chat-7B       | 7B   | 0.763   | 0.360   | 0.562   | 0.400    | 0.367   | 0.265     | 0.344   | 0.548   | 0.307   | 0.379      | 0.411   | 0.439   |
| Qwen-14B           | 14B  | 0.805   | 0.421   | 0.613   | 0.481    | 0.350   | 0.385     | 0.405   | 0.754   | 0.608   | 0.717      | 0.693   | 0.570   |
| Qwen-Chat-14B      | 14B  | 0.814   | 0.442   | 0.628   | 0.382    | 0.400   | 0.350     | 0.377   | 0.732   | 0.478   | 0.736      | 0.649   | 0.551   |
| ChatGLM2-6B        | 6B   | 0.747   | 0.313   | 0.530   | 0.285    | 0.300   | 0.357     | 0.314   | 0.657   | 0.454   | 0.671      | 0.594   | 0.479   |
| Baichuan2-7B-Base  | 7B   | 0.672   | 0.340   | 0.506   | 0.342    | 0.490   | 0.480     | 0.437   | 0.739   | 0.619   | 0.751      | 0.703   | 0.549   |
| Baichuan2-7B-Chat  | 7B   | 0.757   | 0.402   | 0.579   | 0.425    | 0.475   | 0.323     | 0.408   | 0.725   | 0.648   | 0.732      | 0.702   | 0.563   |
| Baichuan2-13B-Base | 13B  | 0.781   | 0.330   | 0.555   | 0.436    | 0.496   | 0.477     | 0.470   | 0.725   | 0.503   | 0.747      | 0.658   | 0.561   |
| Baichuan2-13B-Chat | 13B  | 0.797   | 0.314   | 0.556   | 0.472    | 0.507   | 0.387     | 0.455   | 0.739   | 0.634   | 0.746      | 0.706   | 0.572   |
| InternLM-7B        | 7B   | 0.612   | 0.233   | 0.423   | 0.266    | 0.311   | 0.328     | 0.302   | 0.378   | 0.336   | 0.379      | 0.364   | 0.363   |
| InternLM-7B-Chat   | 7B   | 0.632   | 0.261   | 0.447   | 0.272    | 0.364   | 0.399     | 0.345   | 0.363   | 0.270   | 0.353      | 0.329   | 0.374   |
| InternLM-20B       | 20B  | 0.809   | 0.358   | 0.583   | 0.500    | 0.427   | 0.417     | 0.448   | 0.706   | 0.653   | 0.728      | 0.695   | 0.575   |
| InternLM-20B-Chat  | 20B  | 0.488   | 0.362   | 0.425   | 0.323    | 0.327   | 0.370     | 0.340   | 0.706   | 0.578   | 0.762      | 0.662   | 0.476   |
| CFGPT1-stf-LoRA    | 7B   | 0.820   | 0.414   | 0.617   | 0.569    | 0.729   | 0.769     | 0.689   | 0.745   | 0.584   | 0.609      | 0.646   | 0.650   |
| CFGPT1-sft-Full    | 7B   |**0.836**|**0.476**|**0.656**|**0.700** |**0.808**|**0.829**  |**0.779**|**0.798**|**0.669**|**0.808**   |**0.758**|**0.731**|

More details can be found in [CFBenchmark-Basic](https://github.com/TongjiFinLab/CFBenchmark)

# è‡´è°¢

CFGPTå·²å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬è¦å‘è¿™äº›é¡¹ç›®çš„ç ”ç©¶è€…è¡¨ç¤ºæ„Ÿè°¢å’Œå°Šé‡ã€‚

- InternLM: https://github.com/InternLM/InternLM
- Firefly: https://github.com/yangjianxin1/Firefly
- FinGPT: https://github.com/AI4Finance-Foundation/FinGPT


# æœªæ¥å·¥ä½œ
- [ ] ä½¿ç”¨CFGPTåˆ›å»ºä¸‹æ¸¸çš„åº”ç”¨CFAPP
- [ ] æ„å»ºæ›´åŠ å…¨é¢çš„è®­ç»ƒä»»åŠ¡ä¸å¯¹åº”æ•°æ®
- [ ] æŒç»­æ€§æ”¹è¿›CFGPTåœ¨æ›´å¤šå¤æ‚é‡‘èä»»åŠ¡ä¸Šçš„èƒ½åŠ›

# ä½¿ç”¨è®¸å¯
CFGPTæ˜¯ä¸€ä¸ªç ”ç©¶é¢„è§ˆç‰ˆæœ¬ï¼Œä»…ä¾›éå•†ä¸šç”¨é€”ï¼Œå—InternLMæ¨¡å‹è®¸å¯è¯å’ŒOpenAIç”Ÿæˆæ•°æ®çš„ä½¿ç”¨æ¡æ¬¾çš„çº¦æŸã€‚å¦‚æœæ‚¨å‘ç°ä»»ä½•æ½œåœ¨çš„è¿è§„è¡Œä¸ºï¼Œè¯·ä¸æˆ‘ä»¬è”ç³»ã€‚è¯¥ä»£ç åœ¨Apacheè®¸å¯è¯2.0ä¸‹å‘å¸ƒã€‚

### æ„Ÿè°¢æˆ‘ä»¬çš„è´¡çŒ®è€… :
<a href="https://github.com/TongjiFinLab/CFGPT/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=TongjiFinLab/CFGPT" />
</a>

# å¼•ç”¨
å¦‚æœæ‚¨ä½¿ç”¨ [**CFGPT**](https://arxiv.org/abs/2309.10654) çš„ä»£ç æˆ–æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å£°æ˜å¼•ç”¨ï¼š

```
@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```